{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Exercise 1: Optimize lepton selection\n",
    "\n",
    "* First, print the distributions of the relevant variables for *all* the Monte Carlo samples (i.e. all the *channels* of the $Z$-boson decay to be studied). Which variables are these? Give sensible ranges to include all the events in the samples (both MC and OPAL data) \n",
    "* Do the same for **one** of the OPAL data samples (your lab assistant will decide which one you choose).\n",
    "* Describe the results.\n",
    "* Optimize the object selection by applying cuts. Make a strategy on how to proceed to find the optimal selection. which information do you need?\n",
    "* Determine the efficiency and the amount of background for each $Z$ decay channel. Use the simulated events $e^+e^-$, $\\mu^+\\mu^-$, $\\tau^+\\tau^-$ and hadrons ($qq$). Represent the result in a matrix form and think carefully about how you have to correct the measured rates. Don't forget to calculate the errors!\n",
    "* How do we estimate the statistical fluctuations per bin?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Import libraries**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "import uproot\n",
    "import awkward as ak\n",
    "import mplhep\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scipy.constants as sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Open Monte Carlo samples**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We will open data and Monte Carlo samples using **uproot**. Uproot is a reader and a writer of the ROOT file format using only Python and Numpy. Unlike PyROOT and root_numpy, uproot does not depend on C++ ROOT so that no local compilation of the ROOT libraries is needed to access the data.\n",
    "\n",
    "You can find more info on uproot following the references:\n",
    "* Github repo: https://github.com/scikit-hep/uproot4\n",
    "* Tutorial: https://masonproffitt.github.io/uproot-tutorial/\n",
    "* Video tutorial on uproot and awkward arrays:  https://www.youtube.com/embed/ea-zYLQBS4U \n",
    "\n",
    "First, let's specify the folder path for both data and Monte Carlo (MC) samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Read the Monte Carlo data\n",
    "path_mc = 'opal_data/mc/'\n",
    "\n",
    "# Open the files\n",
    "file_mc_ee = uproot.open(path_mc+'ee.root')\n",
    "file_mc_mm = uproot.open(path_mc+'mm.root')\n",
    "file_mc_tt = uproot.open(path_mc+'tt.root')\n",
    "file_mc_qq = uproot.open(path_mc+'qq.root')\n",
    "\n",
    "\n",
    "# Name the ttree name\n",
    "ttree_name = 'myTTree'\n",
    "\n",
    "# Print list of 'branches' of the TTree (i.e. list of variable names)\n",
    "variable_names = file_mc_ee[ttree_name].keys()\n",
    "\n",
    "# Load branches\n",
    "branches_ee = file_mc_ee[ttree_name].arrays()\n",
    "branches_mm = file_mc_mm[ttree_name].arrays()\n",
    "branches_tt = file_mc_tt[ttree_name].arrays()\n",
    "branches_qq = file_mc_qq[ttree_name].arrays()\n",
    "branches_tot = ak.concatenate((branches_ee,\n",
    "                               branches_mm,\n",
    "                               branches_tt,\n",
    "                               branches_qq))\n",
    "\n",
    "\n",
    "# convert data into dictionaries of numpy arrays\n",
    "array_ee = {}\n",
    "array_mm = {}\n",
    "array_tt = {}\n",
    "array_qq = {}\n",
    "array_tot = {}\n",
    "\n",
    "for variable in variable_names:\n",
    "    array_ee[variable] = ak.to_numpy(branches_ee[variable])\n",
    "    array_mm[variable] = ak.to_numpy(branches_mm[variable])\n",
    "    array_tt[variable] = ak.to_numpy(branches_tt[variable])\n",
    "    array_qq[variable] = ak.to_numpy(branches_qq[variable])\n",
    "    array_tot[variable] = ak.to_numpy(branches_tot[variable])\n",
    "\n",
    "all_channels = ['tot', 'ee', 'mm', 'tt', 'qq']\n",
    "dist_channels = ['ee', 'mm', 'tt', 'qq']\n",
    "array = {'ee' : array_ee,\n",
    "         'mm' : array_mm,\n",
    "         'tt' : array_tt,\n",
    "         'qq' : array_qq,\n",
    "         'tot' : array_tot}\n",
    "\n",
    "\n",
    "# print('These are the different variables: ', variable_names)\n",
    "# print('E_lep = {0:4.2f} GeV = const.'.format(branches_ee['E_lep'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "| Variable name | Description |\n",
    "| --- | --- | \n",
    "| run| Run number |\n",
    "| event | Event number |\n",
    "| Ncharged | Number of charged tracks |\n",
    "| Pcharged | Total scalar sum of track momenta |\n",
    "| E_ecal| Total energy measured in the electromagnetic calorimeter |\n",
    "| E_hcal | Total energy measured in the hadronic calorimeter |\n",
    "| E_lep | LEP beam energy (=$\\sqrt{s}/2$) |\n",
    "| cos_thru | cosine of the polar angle between beam axis and thrust axis |\n",
    "| cos_thet | cosine of the polar angle between incoming positron and outgoing positive particle |\n",
    "\n",
    "\n",
    "For our statistical analysis the run and event number are not interesting. The beam energy $E_\\mathrm{lep}=45.64\\,\\mathrm{GeV}$ is constant for the data set. The scattering angles will not be used for identifying the channels and are subsect of analysis further below. Therefore we first concentrate only on the subset of variables Ncharged, Pcharged, E_ecal and E_hcal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
   ],
   "source": [
    "### We define set of variables of interest\n",
    "### and corresponding dictionaries of plot settings, etc.\n",
    "variables = ['Ncharged', 'Pcharged', 'E_ecal', 'E_hcal']\n",
    "\n",
    "# units corresponding to the variables\n",
    "unit = {'Ncharged' : '',\n",
    "         'Pcharged' : ' [GeV]',\n",
    "         'E_ecal' : ' [GeV]',\n",
    "         'E_hcal' : ' [GeV]'}\n",
    "\n",
    "# bins for histograms\n",
    "bins_mc = {'Ncharged' : np.linspace(0, 40, 41),\n",
    "           'Pcharged' : np.linspace(0, 120, 201),\n",
    "           'E_ecal' : np.linspace(0, 120, 201),\n",
    "           'E_hcal' : np.linspace(0, 40, 201)}\n",
    "\n",
    "# limits for the plots\n",
    "ylims_mc = {'Ncharged' : (0, 1e4),\n",
    "            'Pcharged' : (0, 1e4),\n",
    "            'E_ecal' : (0, 1e4),\n",
    "            'E_hcal' : (0, 1e4)}\n",
    "\n",
    "\n",
    "### Dictionaries concerning different channels\n",
    "plot_label = {'ee' : r'$e$',\n",
    "              'mm' : r'$\\mu$',\n",
    "              'tt' : r'$\\tau$',\n",
    "              'qq' : r'$q$',\n",
    "              'tot' : 'sum'}\n",
    "\n",
    "color = {'ee' : 'tab:blue',\n",
    "         'mm' : 'tab:orange',\n",
    "         'tt' : 'tab:green',\n",
    "         'qq' : 'tab:red',\n",
    "         'tot' : 'grey'}\n",
    "\n",
    "\n",
    "### plotting\n",
    "plt.style.use(mplhep.style.ATLAS)  # plot style of ATLAS\n",
    "fig, axes = plt.subplots(1, 4, figsize=(28, 5))\n",
    "\n",
    "# plot histograms for all variables and channels\n",
    "for (i, variable) in zip(range(4), variables):\n",
    "    for channel in all_channels:\n",
    "        axes[i].hist(array[channel][variable],\n",
    "                     bins=bins_mc[variable],\n",
    "                     histtype='step',\n",
    "                     linewidth=2,\n",
    "                     color=color[channel],\n",
    "                     label=plot_label[channel])\n",
    "\n",
    "\n",
    "    # plot settings\n",
    "    axes[i].set_ylim(ylims_mc[variable])\n",
    "    axes[i].set_title(variable)\n",
    "    axes[i].set_xlabel(variable + unit[variable])\n",
    "    axes[i].set_ylabel(r'# events $N$')\n",
    "    axes[i].legend()\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In the histograms above we see different kind of distributions for different variables and decay channels. Some distributions closely resemble a Gaussian such that a cut can easily be defined via mean and standard deviation. If for example we choose an interval of 3$\\sigma$ we keep approximately 99% of the events. Since the distributions partially overlap, this also causes many false positive events. We hope to make the filters unique by applying such cuts on multiple variables.\n",
    "\n",
    "Other distributions are not easily approximated by a Guassian. These are e.g. the Ncharged curves for $e$, $\\mu$ and $\\tau$, since they have a small mean. Also Pcharged is problematic since often the momentum is missing in the data (is this interpretation correct?), e.g., for $e$ about 50% of the events have Pcharged=0, while the rest of the distribution is centred at higher values.\n",
    "\n",
    "\n",
    "All these effects have to be considered to define meaningful cuts. Nevertheless, let us start the analysis by calculating the mean and standard deviation of all distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# data structure for mean values\n",
    "mean = {'ee' : {},\n",
    "        'mm' : {},\n",
    "        'tt' : {},\n",
    "        'qq' : {}}\n",
    "\n",
    "# data structure for standard deviation\n",
    "std = {'ee' : {},\n",
    "       'mm' : {},\n",
    "       'tt' : {},\n",
    "       'qq' : {}}\n",
    "\n",
    "# auxillary data structure for masks and masked arrays\n",
    "aux_mask = {'ee' : {},\n",
    "            'mm' : {},\n",
    "            'tt' : {},\n",
    "            'qq' : {}}\n",
    "\n",
    "aux_array = {'ee' : {},\n",
    "             'mm' : {},\n",
    "             'tt' : {},\n",
    "             'qq' : {}}\n",
    "\n",
    "\n",
    "# calculate values for all variables and channels\n",
    "for var in variables:\n",
    "    for ch in dist_channels:\n",
    "        # aux array is copied from regular array\n",
    "        aux_array[ch][var] = array[ch][var]\n",
    "\n",
    "        # Pcharged exhibits unexpected behavior:\n",
    "        # a significant number of values is 0.\n",
    "        # there are few outliers with very high values distorting the statistics\n",
    "        # therefore consider only 'regular' events to describe the distributions\n",
    "        if var == 'Pcharged':\n",
    "            # apply lower and upper cut\n",
    "            aux_mask[ch][var] = (array[ch][var] > 0.)\n",
    "            aux_mask[ch][var] *= (array[ch][var] < 120.)\n",
    "            aux_array[ch][var] = aux_array[ch][var][aux_mask[ch][var]]\n",
    "\n",
    "        # calculate statistics of possibly masked arrays\n",
    "        mean[ch][var] = aux_array[ch][var].mean()\n",
    "        std[ch][var] = aux_array[ch][var].std()\n",
    "\n",
    "\n",
    "    # print as a table\n",
    "    print(var)\n",
    "    print('    mean  +-   std, [min,      max]')\n",
    "    for ch in dist_channels:\n",
    "        print('{0}: {1:5.2f} +- {2:5.2f}, [{3:5.2f}, {4:6.2f}]'.format(\n",
    "                ch, mean[ch][var], std[ch][var],\n",
    "                aux_array[ch][var].min(), aux_array[ch][var].max()))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Define cuts:**\n",
    "\n",
    "Apply cuts to the different distributions. Individual notes to the different cut selections are commented.\n",
    "The general reference where to apply a cut is: mean +- 3 sigma, to include most of all events in our selection.\n",
    "In cases of specific asymmetric distributions or something similar, a correction by eye is performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "### define cuts\n",
    "cuts = {'ee' : {},\n",
    "        'mm' : {},\n",
    "        'tt' : {},\n",
    "        'qq' : {}}\n",
    "\n",
    "cuts['ee'] = {'Ncharged' : (0, 6),\n",
    "              'Pcharged' : (mean['ee']['Pcharged'] - 3*std['ee']['Pcharged'], 100),\n",
    "              'E_ecal' : (mean['ee']['E_ecal'] - 3*std['ee']['E_ecal'],\n",
    "                          mean['ee']['E_ecal'] + 3*std['ee']['E_ecal']),\n",
    "              'E_hcal' : (0, mean['ee']['E_hcal'] + 3*std['ee']['E_hcal'])}\n",
    "\n",
    "cuts['mm'] = {'Ncharged' : (0, 4),\n",
    "              'Pcharged' : (mean['mm']['Pcharged'] - 3*std['mm']['Pcharged'],\n",
    "                            mean['mm']['Pcharged'] + 3*std['mm']['Pcharged']),\n",
    "              'E_ecal' : (0, mean['mm']['E_ecal'] + 3*std['mm']['E_ecal']),\n",
    "              'E_hcal' : (0, mean['mm']['E_hcal'] + 3*std['mm']['E_hcal'])}\n",
    "\n",
    "cuts['tt'] = {'Ncharged' : (0, 7),\n",
    "              'Pcharged' : (mean['tt']['Pcharged'] - 2*std['tt']['Pcharged'],\n",
    "                            mean['tt']['Pcharged'] + 3*std['tt']['Pcharged'] - 15),\n",
    "              'E_ecal' : (0, mean['tt']['E_ecal'] + 3*std['tt']['E_ecal'] - 5),\n",
    "              'E_hcal' : (0, mean['tt']['E_hcal'] + 3*std['tt']['E_hcal'])}\n",
    "\n",
    "cuts['qq'] = {'Ncharged' : (7, 39),\n",
    "              'Pcharged' : (mean['qq']['Pcharged'] - 2*std['qq']['Pcharged'] - 10,\n",
    "                            mean['qq']['Pcharged'] + 2*std['qq']['Pcharged'] - 4),\n",
    "              'E_ecal' : (mean['qq']['E_ecal'] - 3*std['qq']['E_ecal'],\n",
    "                          mean['qq']['E_ecal'] + 3*std['qq']['E_ecal'] - 10),\n",
    "              'E_hcal' : (0, mean['qq']['E_hcal'] + 3*std['qq']['E_hcal'])}\n",
    "\n",
    "\n",
    "# print as a table\n",
    "for var in variables:\n",
    "    print(var)\n",
    "    print('    left cut,  right cut')\n",
    "    for ch in dist_channels:\n",
    "        print('{0}: {1:7.2f} , {2:10.2f}'.format(\n",
    "                ch,\n",
    "                cuts[ch][var][0],\n",
    "                cuts[ch][var][1]))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Tool to adjust Cuts:**\n",
    "\n",
    "Here we can specify which data and which cuts should be shown. This allows to visualize nicely where cuts should be changed in order to minimize e.g. a specific off diagonal element of the efficiency matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "### Plotting only specific channels and cuts for all variables\n",
    "spec_channels = ['tot', 'ee', 'mm', 'tt', 'qq']\n",
    "spec_cuts = ['ee']\n",
    "\n",
    "print(f'data from: {spec_channels}')\n",
    "print(f'cuts from: {spec_cuts}')\n",
    "\n",
    "\n",
    "### plotting\n",
    "plt.style.use(mplhep.style.ATLAS)  # plot style of ATLAS\n",
    "fig, axes = plt.subplots(1, 4, figsize=(28, 5))\n",
    "\n",
    "# plot histograms for all variables and channels\n",
    "for (i, variable) in zip(range(4), variables):\n",
    "    for channel in spec_channels:\n",
    "        axes[i].hist(array[channel][variable],\n",
    "                     bins=bins_mc[variable],\n",
    "                     histtype='step',\n",
    "                     linewidth=2,\n",
    "                     alpha= 0.9,\n",
    "                     color=color[channel],\n",
    "                     label=plot_label[channel])\n",
    "\n",
    "    for cut in spec_cuts:\n",
    "        axes[i].axvline(cuts[cut][variable][0], ls='--', lw=3, color=color[cut])\n",
    "        axes[i].axvline(cuts[cut][variable][1], ls='--', lw=3, color=color[cut],\n",
    "                        label=f'cut: {plot_label[cut]}')\n",
    "\n",
    "\n",
    "    # plot settings\n",
    "    axes[i].set_ylim(ylims_mc[variable])\n",
    "    axes[i].set_title(variable)\n",
    "    axes[i].set_xlabel(variable + unit[variable])\n",
    "    axes[i].set_ylabel(r'# events $N$')\n",
    "    axes[i].legend()\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In the plots, specific cuts in combination with individual data can be observed.\n",
    "- The parameter \"spec_channels\" specifies the data sets that will be plotted\n",
    "- The parameter 'spec_cuts' specifies the specific particle types (ee, mm, tt or qq) for which the selection cuts will be shown\n",
    "\n",
    "This piece of code can be used to play a bit around with the data sets and the cuts.\n",
    "\n",
    "It can be observed, that:\n",
    "- Ncharged is a good indicator to seperate the hadronic from the leptonic channels\n",
    "\n",
    "- Pcharged is generally a good indicator to distinguish between mm and tt decays.\n",
    "- The problem here is, that in many cases the momenta were not tracked correctly, resulting in a tracked value of Pcharged=0.\n",
    "- The mm- respectively tt- Pcharged=0 events can not be distinguished.\n",
    "- As there is no other effective criterion to prevent mm-events to be classified as tt-events, the selection rate here is the worst in our whole data.\n",
    "- It is possible to prevent tt-events to be classified as mm-events via Ncharged. Unfortunately this is not possible the other way round.\n",
    "\n",
    "- Eecal allows us to seperate ee and mm events very effectively\n",
    "\n",
    "- Hcal can be used to avoid mistakenly assigning electrons to other events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def identify(array_xx, channel):\n",
    "    '''Returns a mask designed to identify events of channel in array_xx'''\n",
    "    mask = True\n",
    "    for variable in variables:\n",
    "        # lower and upper cut\n",
    "        mask *= array_xx[variable] >= cuts[channel][variable][0]\n",
    "        mask *= array_xx[variable] <= cuts[channel][variable][1]\n",
    "        # to include data with artifacts in 'Pcharged'\n",
    "        # this does not ocurr for hadrons\n",
    "        if variable=='Pcharged' and channel in ['ee', 'mm', 'tt']:\n",
    "            mask += array_xx['Pcharged'] == 0\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# calculate efficiency matrix\n",
    "filter_eff = np.zeros((4, 4))\n",
    "filter_error = np.zeros((4, 4))\n",
    "n_mc = np.zeros((4, 4))\n",
    "N_mc = np.zeros(4)\n",
    "\n",
    "for (j, col_ch) in zip(range(4), dist_channels):\n",
    "    N_mc[j] = 100000  # len(array[col_ch]['Pcharged'])  # 100000?\n",
    "    for (i, row_ch) in zip(range(4), dist_channels):\n",
    "        n_mc[i, j] = sum(identify(array[col_ch], row_ch))\n",
    "\n",
    "\n",
    "filter_eff = n_mc / N_mc[None,:]\n",
    "filter_error = np.sqrt((n_mc+1)*(n_mc+2)/(N_mc[None,:]+2)/(N_mc[None,:]+3) - (n_mc+1)**2/(N_mc[None,:]+2)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "print(\"efficiency matrix: \\n\", np.array_repr(filter_eff, precision=7, suppress_small=True), '\\n \\n',\n",
    "      \"errors of efficiency matrix: \\n\", np.array_repr(filter_error, precision=7, suppress_small=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Matrix Inversion\n",
    "To determine the uncertainties of the matrix elements after the inversion we diffierent methods and compare:\n",
    "- analytical formula (see reference below)\n",
    "- generate random efficiency matrices, invert all and determine the statistics via:\n",
    "  * direct formula of mean and std\n",
    "  * Gaussian fit to each matrix element\n",
    "\n",
    "**References**:\n",
    "* Propagation of Errors for Matrix Inversion: https://arxiv.org/abs/hep-ex/9909031v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "### invert matrix and propagate errors\n",
    "inv_filter_eff = np.linalg.inv(filter_eff)\n",
    "inv_filter_error = np.sqrt(np.linalg.multi_dot([inv_filter_eff**2,\n",
    "                                                filter_error**2,\n",
    "                                                inv_filter_eff**2]))\n",
    "\n",
    "# covariance matrix of the matrix elements\n",
    "inv_filter_cov = np.zeros((4, 4, 4, 4))\n",
    "\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        for k in range(4):\n",
    "            for l in range(4):\n",
    "                left_vec = (inv_filter_eff[i, :]*inv_filter_eff[k, :])\n",
    "                right_vec = (inv_filter_eff[:, j]*inv_filter_eff[:, l])\n",
    "                inv_filter_cov[i, j, k, l] = left_vec @ filter_error[:, :]**2 @ right_vec\n",
    "\n",
    "\n",
    "print(inv_filter_cov[0, :, 3, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "### Number of toy experiments to be done\n",
    "ntoy = 1000\n",
    "\n",
    "### Create numpy matrix of list to append elements of inverted toy matrices\n",
    "inverse_toys = np.empty((4,4))\n",
    "\n",
    "# Create toy efficiency matrix out of gaussian-distributed random values\n",
    "for i in range(0,ntoy,1):\n",
    "    toy_matrix = np.zeros((4,4))\n",
    "    toy_matrix = np.random.normal(filter_eff,\n",
    "                                  filter_error,\n",
    "                                  size=(4,4))\n",
    "    \n",
    "    # Invert toy matrix\n",
    "    inverse_toy = np.linalg.inv(toy_matrix)\n",
    "    \n",
    "    # Append values\n",
    "    inverse_toys = np.dstack((inverse_toys,inverse_toy))\n",
    "\n",
    "### calculate the statistics directly\n",
    "inv_filter_eff_stat = np.mean(inverse_toys, axis=2)\n",
    "inv_filter_error_stat = np.std(inverse_toys, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "### Get statistics of toy experiments from Gaussian fit\n",
    "\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "\n",
    "inv_filter_eff_fit = np.zeros((4,4))\n",
    "inv_filter_error_fit = np.zeros((4,4))\n",
    "\n",
    "# Define gaussian function to fit to the toy distributions:\n",
    "def gauss(x, A, mu, sigma):\n",
    "    return A*np.exp(-(x-mu)**2/(2.*sigma**2))\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(20, 10), dpi=80)\n",
    "fig.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.2, hspace=0.2)\n",
    "ax00 = plt.subplot(4,4,1)\n",
    "ax01 = plt.subplot(4,4,2)\n",
    "ax02 = plt.subplot(4,4,3)\n",
    "ax03 = plt.subplot(4,4,4)\n",
    "\n",
    "ax10 = plt.subplot(4,4,5)\n",
    "ax11 = plt.subplot(4,4,6)\n",
    "ax12 = plt.subplot(4,4,7)\n",
    "ax13 = plt.subplot(4,4,8)\n",
    "\n",
    "ax20 = plt.subplot(4,4,9)\n",
    "ax21 = plt.subplot(4,4,10)\n",
    "ax22 = plt.subplot(4,4,11)\n",
    "ax23 = plt.subplot(4,4,12)\n",
    "\n",
    "ax30 = plt.subplot(4,4,13)\n",
    "ax31 = plt.subplot(4,4,14)\n",
    "ax32 = plt.subplot(4,4,15)\n",
    "ax33 = plt.subplot(4,4,16)\n",
    "\n",
    "axes = [[ax00,ax01,ax02,ax03],\n",
    "        [ax10,ax11,ax12,ax13],\n",
    "        [ax20,ax21,ax22,ax23],\n",
    "        [ax30,ax31,ax32,ax33]]\n",
    "\n",
    "## Find suitable ranges to fit/plot gaussian distributions\n",
    "ranges = np.zeros((4, 4, 2))\n",
    "ranges[:, :, 0] =  inv_filter_eff - 4*inv_filter_error\n",
    "ranges[:, :, 1] =  inv_filter_eff + 4*inv_filter_error\n",
    "\n",
    "## Find suitable initial parameters for gaussian distributions\n",
    "p0s = np.zeros((4, 4, 3))\n",
    "p0s[:, :, 0] = 10000\n",
    "p0s[:, :, 1] = inv_filter_eff\n",
    "p0s[:, :, 2] = inv_filter_error\n",
    "\n",
    "\n",
    "# Fill histograms for each inverted matrix coefficient:\n",
    "for j in range(0, 4):\n",
    "    for k in range(0, 4):\n",
    "\n",
    "        # Diagonal and off-diagonal terms have different histogram ranges\n",
    "        hbins, hedges, _ = axes[j][k].hist(inverse_toys[j,k,:],\n",
    "                                           bins=30,\n",
    "                                           range=ranges[j, k, :],\n",
    "                                           histtype='step',\n",
    "                                           linewidth=2,\n",
    "                                           label=f'toyhist{j}{k}')\n",
    "        axes[j][k].legend()\n",
    "\n",
    "\n",
    "        # Get the fitted curve\n",
    "        h_mid = 0.5*(hedges[1:] + hedges[:-1]) #Calculate midpoints for the fit\n",
    "        coeffs, _ = curve_fit(gauss, h_mid, hbins, p0=p0s[j, k], maxfev=10000)\n",
    "        h_fit = gauss(h_mid, *coeffs)\n",
    "\n",
    "        inv_filter_eff_fit[j, k] = coeffs[1]\n",
    "        inv_filter_error_fit[j, k] = coeffs[2]\n",
    "\n",
    "        axes[j][k].plot(h_mid, h_fit,label=f'Fit{j}{k}')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "### comparison\n",
    "print(\"inverse matrix: \\n\", np.array_repr(inv_filter_eff, precision=7, suppress_small=True), '\\n')\n",
    "print(\"analytical errors: \\n\", np.array_repr(inv_filter_error, precision=7, suppress_small=True), '\\n')\n",
    "print(\"direct statistical errors: \\n\", np.array_repr(inv_filter_error_stat, precision=7, suppress_small=True), '\\n')\n",
    "print(\"fit errors: \\n\", np.array_repr(inv_filter_error_fit, precision=7, suppress_small=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The analytical expression is exact as discussed in the reference. We see that the fit method closely matches these results. The other method however does not. Why is that so? We do not know. BUT: Since the analytical expression is exact, we do not have to find the reason. We stick with the exact propagation formula."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Exercise 2: Separate $t$- and $s$-channel contributions\n",
    "\n",
    "Only Feynman diagrams contributing to the production of $Z$ boson are to be considered for the measurements. The **electron** Monte Carlo sample incorporate contributions from $t$- and $s$-channels.\n",
    "* Select/correct contributions producing $Z$ boson decays. (Hint: Which role does the $\\cos(\\theta)$ distribution play in separating $t$- and $s$-channels?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# As the t-channel only exists for ee-events, only these are considered in ex.2\n",
    "\n",
    "### We define set of angle variables\n",
    "# add definitions to dictionaries from above\n",
    "angle_variables = ['cos_thru', 'cos_thet']\n",
    "\n",
    "# units corresponding to these variables\n",
    "unit['cos_thru'] = ''\n",
    "unit['cos_thet'] = ''\n",
    "\n",
    "# bins for histograms\n",
    "bins_mc['cos_thru'] = np.linspace(-1, 1, 201)\n",
    "bins_mc['cos_thet'] = np.linspace(-1, 1, 201)\n",
    "\n",
    "# limits for the plots\n",
    "ylims_mc['cos_thru'] = (0, 2e3)\n",
    "ylims_mc['cos_thet'] = (0, 2e3)\n",
    "\n",
    "# Function that describes the s-channel distribution\n",
    "def s_distribution(cos_thet, A):\n",
    "    return A*(1+cos_thet**2)\n",
    "\n",
    "# function that describes the t-channel\n",
    "def t_distribution(cos_thet, A):\n",
    "    return A*(1-cos_thet)**-2\n",
    "\n",
    "### plotting\n",
    "plt.style.use(mplhep.style.ATLAS)  # plot style of ATLAS\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# plot histograms for the cos_thet and cos_thru distribution of the ee-events\n",
    "for (i, variable) in zip(range(2), angle_variables):\n",
    "    for channel in ['ee']:\n",
    "        axes[i].hist(array[channel][variable],\n",
    "                     bins=bins_mc[variable],\n",
    "                     histtype='step',\n",
    "                     linewidth=2,\n",
    "                     color=color[channel],\n",
    "                     label=plot_label[channel])\n",
    "\n",
    "    # plot settings\n",
    "    axes[i].set_ylim(ylims_mc[variable])\n",
    "    axes[i].set_title(variable)\n",
    "    axes[i].set_xlabel(variable + unit[variable])\n",
    "    axes[i].set_ylabel(r'# events $N$')\n",
    "    axes[i].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Cite Paper: Precise Determination of the Z Resonance Parameters at LEP: \"Zedometry\"\n",
    "# p.23 (ch. 7.2.1 t-channel contribution to ee -> ee)\n",
    "# Some key points:\n",
    "# - t-channel amplitude is non-resonant, dominated by photon exchange\n",
    "# - relative size of t-channel amplitude depends on the scattering angle (theta)\n",
    "# - cut is made at |cos(theta)| < 0.7\n",
    "# - resulting from high-statistics Monte Carlo samples\n",
    "# OPAL_Paper_3 results in the same\n",
    "\n",
    "# Apply the cut at |cos(theta)| < 0.7 to seperate s- and t-channels for ee-events\n",
    "cuts['ee']['cos_thet'] = (-0.7, 0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The relative size of the t-channel amplitude depends on cos_theta and becomes the major component at high values of this variable.\n",
    "To make sure the polar angle lies well within the barrel region of the detector, a cut is made at |cos(theta)| < 0.7 (as explained in the paper \"Zedometry\").\n",
    "In order to enhance the s-channel component of the selected data sample, a linear combination of the s-and t-channel distributions is fitted to the data. Then, only the s-channel part is plotted, as well.\n",
    "This s-channel curve (i.e. the area below this curve in the range[-1,1]) indicates now, how many events correspond to the s-channel. This number of events can be determined by integration (Take the bin width into account!).\n",
    "The s+t-channel curve (i.e. the area below in the range (-0.7,0.7)) indicates how many events we find in our data after the cut. In order to be able to determine the number of real s-channel events in the range [-1,1] using the number of measured counts in the range (-0.7,0.7), we will divide the areas under the curves in the described ranges and thus obtain a correction factor with which we can determine the s-channel-data from our measurement data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# fit a linear combination of s- and t-channel distributions\n",
    "def s_t_distribution(cos_thet, A, B):\n",
    "    '''Linear combination of s- and t-channel distributions'''\n",
    "    return A*(1+cos_thet**2) + B*(1-cos_thet)**-2\n",
    "\n",
    "\n",
    "# function to integrate the s+t channel function in the range (-0.7,0.7)\n",
    "# the coefficients were determined by wolframAlpha\n",
    "def s_t(A, B):\n",
    "    return A * 1.62867 + B * 2.7451\n",
    "\n",
    "\n",
    "# define variable and channel\n",
    "variable = 'cos_thet'\n",
    "channel = 'ee'\n",
    "\n",
    "###plotting\n",
    "fig, ax = plt.subplots()\n",
    "# histogram the data\n",
    "hbins, hedges, _ = ax.hist(array[channel][variable], bins=np.linspace(-1,1,201),\n",
    "                        histtype='step', linewidth=2, color='C0',\n",
    "                        label=plot_label[channel])\n",
    "h_mid = 0.5*(hedges[1:] + hedges[:-1])  # Calculate midpoints for the fit\n",
    "ax.errorbar(h_mid[10:-10:10], hbins[10:-10:10], yerr=np.sqrt(hbins[10:-10:10]),\n",
    "            c='black', fmt='none', zorder=5, capsize=2, label='stat. uncertainty')\n",
    "\n",
    "# perform the fit\n",
    "coeffs, cov = curve_fit(s_t_distribution, h_mid[10:-10], hbins[10:-10],\n",
    "                        sigma=np.sqrt(hbins[10:-10]), absolute_sigma=True,\n",
    "                        maxfev=10000)  # fit in between -0.895 and 0.895\n",
    "\n",
    "# plot the s+t channel fit\n",
    "ax.plot(bins_mc[variable], s_t_distribution(bins_mc[variable], coeffs[0], coeffs[1]),\n",
    "        color=\"C2\", label='(s+t) - fit')\n",
    "\n",
    "# plot the s-channel fit\n",
    "ax.plot(bins_mc[variable], s_distribution(bins_mc[variable], coeffs[0]),\n",
    "        color=\"C3\", label='s - fit')\n",
    "\n",
    "\n",
    "# Show the cuts in the graph\n",
    "ax.axvline(cuts[channel][variable][1], ls='--', color = 'C1', label=r'data cuts')\n",
    "ax.axvline(cuts[channel][variable][0], ls='--', color = 'C1')\n",
    "ax.axvline(h_mid[10], ls='--', color = 'black', label=r'fit limits')\n",
    "ax.axvline(h_mid[-10], ls='--', color = 'black')\n",
    "\n",
    "#print('fit edges:', h_mid[10], h_mid[-11])\n",
    "\n",
    "# Plot settings\n",
    "ax.set_ylim(0,1e3)\n",
    "ax.set_title(r'cos_theta-distribution')\n",
    "ax.set_xlabel(r'cos($\\theta$)')\n",
    "ax.set_ylabel(r'# events $N$')\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "# calculate the bin width\n",
    "bin_width = hedges[1]-hedges[0]\n",
    "\n",
    "# How many events do we count in the range (-0.7,0.7)?\n",
    "# Integration\n",
    "A_count = s_t(coeffs[0], coeffs[1]) / bin_width\n",
    "# Error propagation\n",
    "u_A_count = np.sqrt((s_t(cov[0,0], 0))**2\n",
    "                      + s_t(0, cov[1,1])**2\n",
    "                      + 2 * s_t(cov[0,0],0)\n",
    "                      * s_t(0, cov[1,1])*cov[0,1]) / bin_width\n",
    "\n",
    "# How many events do really correspond to the s-channel?\n",
    "# Integration\n",
    "A_real = 8/3 * coeffs[0] / bin_width\n",
    "# Error propagation\n",
    "u_A_real = 8/3 * cov[0,0] / bin_width\n",
    "\n",
    "# Factor #real_events / #counted_events\n",
    "ratio_s_t = A_real / A_count\n",
    "# Error propagation\n",
    "u_ratio_s_t = np.sqrt((u_A_real/A_count)**2 + (A_real/A_count**2 * u_A_count)**2)\n",
    "\n",
    "# Print the results\n",
    "print('A_count =', A_count.round(0), '+-', u_A_count.round(0), ', relative error:', (u_A_count/A_count).round(3))\n",
    "print('A_real =', A_real.round(0), '+-', u_A_real.round(0), ', relative error:', (u_A_real/A_real).round(3))\n",
    "print('Ratio real s-channel data / counted data =', ratio_s_t.round(3), '+-', u_ratio_s_t.round(3), ', relative error:', (u_ratio_s_t/ratio_s_t).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Function to apply the cos_thet cuts to the ee-data\n",
    "def identify_s(array_ee):\n",
    "    '''Returns a mask designed to identify s-channel in array_ee'''\n",
    "    mask = True\n",
    "    mask *= array_ee['cos_thet'] > cuts['ee']['cos_thet'][0]\n",
    "    mask *= array_ee['cos_thet'] < cuts['ee']['cos_thet'][1]\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Plot the Pcharged - distribution for the ee-data with and without a cut in cos_thet\n",
    "\n",
    "# Setup figure\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "variable = 'Pcharged'\n",
    "\n",
    "# create mask without cos_thet cut\n",
    "mask_ee = identify(array['tot'],'ee')\n",
    "# Add the cos_thet cut to a second mask\n",
    "mask_thet_ee = mask_ee * identify_s(array['tot'])\n",
    "\n",
    "# Plot Pcharged without cos_thet cut\n",
    "axes[0].hist(array['tot'][variable][mask_ee],\n",
    "             bins=np.linspace(0,120,201), histtype='step',\n",
    "             linewidth=2, label=r'ee-data without cos$\\theta$ - cut')\n",
    "\n",
    "# Plot Pcharged with cos_thet cut\n",
    "axes[1].hist(array['tot'][variable][mask_thet_ee],\n",
    "             bins=np.linspace(0,120,201), histtype='step',\n",
    "             linewidth=2, label=r'ee-data with cos$\\theta$ - cut')\n",
    "\n",
    "\n",
    "# Plot settings\n",
    "axes[0].set_title(r\"Pcharged without cos$\\theta$ - cut\")\n",
    "axes[1].set_title(r\"Pcharged with cos$\\theta$ - cut\")\n",
    "axes[0].set_xlabel(variable + unit[variable])\n",
    "axes[1].set_xlabel(variable + unit[variable])\n",
    "axes[0].set_ylabel(r'# events $N$')\n",
    "axes[1].set_ylabel(r'# events $N$')\n",
    "axes[0].legend()\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "It can be observed that by applying the cos_theta cut we loose most of the Pcharged=0 events. This is due to the fact, that at small scattering angles the electron flies nearly parallel to the magnetic field, so the momentum cannot be measured."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Exercise 3: Measurement of the total production cross sections\n",
    "\n",
    "For **each** of the seven centre-of-mass energies:\n",
    "* Determine the number of events in the handronic channel *and* in the three leptonic channels\n",
    "* Substract the background and correct for selection efficiencies accordingly\n",
    "* Then, calculate the differnetial cross sections for the hadronic *and* the leptnic channels\n",
    "* Add the radiation corrections from The table given below. **Don't forget to take the uncertainties (errors) into account!**\n",
    "\n",
    "| $\\sqrt{s}$   \\[GeV\\]| Correction hadronic channel    \\[nb\\] |  Correction leptonic channel   \\[nb\\]|\n",
    "| --- | --- | --- |\n",
    "| 88.47 | +2.0  | +0.09 |\n",
    "| 89.46 | +4.3  | +0.20 |\n",
    "| 90.22 | +7.7  | +0.36 |\n",
    "| 91.22 | +10.8 | +0.52 |\n",
    "| 91.97 | +4.7  | +0.22 |\n",
    "| 92.96 | -0.2  | -0.01 |\n",
    "| 93.76 | -1.6  | -0.08 |\n",
    "\n",
    "Feel free to access these values using the dictionary 'xs_corrections' given below.\n",
    "* Once the total cross section for all four decay channels at all seven energies have been measured, fit a **Breit-Wigner distribution** to measure the $Z$ boson mass ($m_Z$) and the resonance width ($\\Gamma_Z$) and the peak cross section s of the resonance for the hadronic and the leptonic channels. Again, **propagate the uncertainties carefully**.\n",
    "* Compare your results to the OPAL cross section s and the theoretical predictions. How many degrees of freedom does the fit have? How can you udge if the model is compatible with the measured data? Calculate the  **confidence levels**.\n",
    "* Calculate the partial widths for all channels from the measured cross sections on the peak. Which is the best partial width to start with? Compare them with the theoretical predictions and the values that you have calculated in the beginning.\n",
    "* Determine from your results the **number of generations of light neutrinos**. Which assumptions are necessary?\n",
    "* Discuss in detail the systematic uncertainties in the whole procedure of the analysis. Which assumptions were necessary?\n",
    "\n",
    "These are some **references** that might be interesting to look up:\n",
    "* Particle Data Book: https://pdg.lbl.gov/2020/download/Prog.Theor.Exp.Phys.2020.083C01.pdf\n",
    "** Resonances: https://pdg.lbl.gov/2017/reviews/rpp2017-rev-resonances.pdf\n",
    "* Precision Electroweak Measurements on the Z Resonance (Combination LEP): https://arxiv.org/abs/hep-ex/0509008\n",
    "* Measurement of the $Z^0$ mass and width with the OPAL detector at LEP: https://doi.org/10.1016/0370-2693(89)90705-3\n",
    "* Measurement of the $Z^0$ line shape parameters and the electroweak couplings of charged leptons: https://inspirehep.net/literature/315269\n",
    "* The OPAL Collaboration, *Precise Determination of the $Z$ Resonance Parameters at LEP: \"Zedometry\"*: https://arxiv.org/abs/hep-ex/0012018\n",
    "* Fitting a Breit-Wigner curve using uproot: https://masonproffitt.github.io/uproot-tutorial/07-fitting/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "xs_corrections = { 'energy' : [88.47, 89.46, 90.22, 91.22, 91.97, 92.96, 93.76] ,\n",
    "                   'hadronic' : [2.0, 4.3, 7.7, 10.8, 4.7, -0.2, -1.6],\n",
    "                   'leptonic' : [0.09, 0.20, 0.36, 0.52, 0.22, -0.01, -0.08]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "### Data management\n",
    "\n",
    "path_data = 'opal_data/data'\n",
    "\n",
    "# Open the files\n",
    "file_lep = uproot.open(path_data+'/daten_1.root')\n",
    "\n",
    "# Name the ttree name\n",
    "ttree_name = 'myTTree'\n",
    "\n",
    "# Load branches\n",
    "branches_lep = file_lep[ttree_name].arrays()\n",
    "\n",
    "# convert data into dictionaries of numpy arrays\n",
    "array_lep = {}\n",
    "\n",
    "for variable in variable_names:\n",
    "    array_lep[variable] = ak.to_numpy(branches_lep[variable])\n",
    "\n",
    "N_data = len(array_lep['E_ecal'])\n",
    "\n",
    "print('Number of events:', N_data)\n",
    "\n",
    "print('These are the different variables: ',\n",
    "      file_lep[ttree_name].keys(),\n",
    "      ' the same as before')\n",
    "\n",
    "print('E_lep = {0:4.2f} GeV = const.'.format(branches_ee['E_lep'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# identify the events of each energy\n",
    "\n",
    "# dictionary for different beam energies\n",
    "array_E_lep = {}\n",
    "E_lep = np.zeros(7)\n",
    "\n",
    "# roughly estimated bin edges\n",
    "E_bin_edges = [44, 44.5, 44.9, 45.3, 45.8, 46.2, 46.7, 47.5]\n",
    "\n",
    "for i in range(7):\n",
    "    array_E_lep[i] = {}  # sub dictionary for different variables\n",
    "    mask = True\n",
    "    mask *= (array_lep['E_lep'] >= E_bin_edges[i])\n",
    "    mask *= (array_lep['E_lep'] <= E_bin_edges[i+1])\n",
    "\n",
    "    # mask arrays of all variables\n",
    "    for variable in variable_names:\n",
    "        array_E_lep[i][variable] = array_lep[variable][mask]\n",
    "\n",
    "    E_lep[i] = np.mean(array_E_lep[i]['E_lep'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# M_all_chs contains vectors of event numbers (of all channels)\n",
    "M_all_chs = np.zeros((7, 4))\n",
    "\n",
    "for i in range(7):\n",
    "    for (j, ch) in zip(range(4), dist_channels):\n",
    "        M_all_chs[i, j] = np.sum(identify(array_E_lep[i], ch))\n",
    "\n",
    "\n",
    "print('E_lep | [ee,     mm,    tt,    qq]')\n",
    "print('------------------------------------')\n",
    "for i in range(7):\n",
    "    print(round(E_lep[i], 2), '|', M_all_chs[i, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# M are vectors of event numbers (only s channel)\n",
    "M = M_all_chs\n",
    "u_M = np.sqrt(M_all_chs)\n",
    "\n",
    "# for ee we have to separate s and t channel\n",
    "# keep only s channel and correct for missing events\n",
    "for i in range(7):\n",
    "    mask_ee_s = identify(array_E_lep[i], 'ee')\n",
    "    mask_ee_s *= identify_s(array_E_lep[i])\n",
    "    M[i, 0] = np.sum(mask_ee_s)\n",
    "    M[i, 0] *= ratio_s_t  # correct for s-channel selection\n",
    "    u_M[i, 0] = M[i, 0] * np.sqrt((u_ratio_s_t/ratio_s_t)**2 + 1/M[i, 0])  # error propagation\n",
    "    \n",
    "    \n",
    "print('E_lep | [ee,     mm,    tt,    qq][stat. uncertainty][rel. uncertainty]')\n",
    "print('--------------------------------------------------------------------------------------')\n",
    "for i in range(7):\n",
    "    print(round(E_lep[i], 2),\n",
    "          '|',\n",
    "          np.round(M[i, :], 0),\n",
    "          np.round(u_M[i, :], 0),\n",
    "          np.round(u_M[i, :]/M[i, :], 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# N are vectors of event numbers corrected for efficiency\n",
    "N = np.zeros((7, 4))\n",
    "N_cov = np.zeros((7, 4, 4))\n",
    "u_N = np.zeros((7, 4))\n",
    "\n",
    "for i in range(7):\n",
    "    N[i, :] = inv_filter_eff @ M[i, :]\n",
    "    for j in range(4):\n",
    "        for k in range(4):\n",
    "            N_cov[i, j, k] = M[i, :] @ inv_filter_cov[j, :, k, :] @ M[i, :]\n",
    "            N_cov[i, j, k] += (inv_filter_eff[j, :] * inv_filter_eff[k, :]) @ u_M[i, :]**2\n",
    "    u_N[i, :] = np.sqrt(np.diagonal(N_cov[i, :, :]))\n",
    "\n",
    "\n",
    "print('E_lep | [ee,     mm,    tt,    qq][stat. uncertainty][rel. uncertainty]')\n",
    "print('--------------------------------------------------------------------------------------')\n",
    "for i in range(7):\n",
    "    print(round(E_lep[i], 2), '|', np.round(N[i, :], 0), np.round(u_N[i, :], 0), np.round(u_N[i, :]/N[i, :], 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "#for i in range(7):\n",
    "#    print(np.array_repr(N_cov[i, :, :], precision=4, suppress_small=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The off diagonal entries are small. further consider only variances, no covariances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# import luminosity file\n",
    "import pandas as pd\n",
    "\n",
    "lumi_df = pd.read_csv('opal_data/lumi_files/daten_1.csv')\n",
    "\n",
    "print('these are the different columns: ', lumi_df.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# calculate cross sections\n",
    "\n",
    "cross_array = N / np.array(lumi_df['lumi'])[:, None]\n",
    "u_cross_array_stat = cross_array * np.sqrt((u_N/N)**2 + np.array(lumi_df['stat']/lumi_df['lumi'])[:, None]**2)\n",
    "u_cross_array_all = cross_array * np.sqrt((u_N/N)**2 + np.array(lumi_df['all']/lumi_df['lumi'])[:, None]**2)\n",
    "\n",
    "cross_array[:, :3] += np.array(xs_corrections['leptonic'])[:, None]\n",
    "cross_array[:, 3] += np.array(xs_corrections['hadronic'])\n",
    "\n",
    "# for consistency go back to dictionaries\n",
    "# no more matrix calculations needed\n",
    "cross_sec = {}\n",
    "u_cross_stat = {}\n",
    "u_cross_all = {}\n",
    "\n",
    "for i, ch in zip(range(4), dist_channels):\n",
    "    cross_sec[ch] = cross_array[:, i]\n",
    "    u_cross_stat[ch] = u_cross_array_stat[:, i]\n",
    "    u_cross_all[ch] = u_cross_array_all[:, i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "### which errors?\n",
    "# fit_sigma = u_cross_stat\n",
    "fit_sigma = u_cross_all\n",
    "\n",
    "### Include errors of points in parameter errors?\n",
    "absolute = True  # in general smaller errors\n",
    "# absolute = False  # in general larger errors\n",
    "\n",
    "def Breit_Wigner(E, M_Z, Gamma_Z, Peak, Underground):\n",
    "    return Peak * (Gamma_Z*E)**2 / ((E**2 - M_Z**2)**2 + (E**2*Gamma_Z/M_Z)**2) + Underground\n",
    "\n",
    "# dictionaries for fit parameters and their covariances\n",
    "coeffs = {}\n",
    "covs = {}\n",
    "u_coeffs = {}\n",
    "chisq = {}\n",
    "\n",
    "# energy values for plotting the fits\n",
    "E_axis = 2*np.linspace(44, 47, 501)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 4, figsize=(28, 5))\n",
    "\n",
    "for ch, i in zip(dist_channels, range(4)):\n",
    "    # plot the cross sections\n",
    "    ax[i].errorbar(x=2*E_lep, y=cross_sec[ch], yerr=fit_sigma[ch],\n",
    "                   fmt='.', c='black', zorder=10,\n",
    "                   label='measurements')\n",
    "\n",
    "    # perform and plot fit\n",
    "    coeffs[ch], covs[ch] = curve_fit(Breit_Wigner, 2*E_lep, cross_sec[ch],\n",
    "                                     p0=[90, 2.5, 3, 0], maxfev=10000,\n",
    "                                     sigma=fit_sigma[ch], absolute_sigma=absolute)\n",
    "    chisq[ch] = np.sum((cross_sec[ch]-Breit_Wigner(2*E_lep, *coeffs[ch]))**2 / fit_sigma[ch]**2)\n",
    "    u_coeffs[ch] = np.sqrt(np.diagonal(covs[ch]))\n",
    "\n",
    "    ax[i].plot(E_axis, Breit_Wigner(E_axis, *coeffs[ch]), c=color[ch],\n",
    "               label=r'Breit Wigner fit: $\\chi^2/\\mathrm{dof}=$'+'{0:4.2f}'.format(chisq[ch]/4))\n",
    "\n",
    "    ax[i].set_title(plot_label[ch] + ' channel')\n",
    "    ax[i].set_xlabel('energy $\\sqrt{s}$ [GeV]')\n",
    "    ax[i].set_ylabel('cross section $\\sigma$ [nb]')\n",
    "\n",
    "    ax[i].legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print('             M_Z             Gamma_Z             Peak             Offset')\n",
    "for ch in dist_channels:\n",
    "    print('{}: {:8.3f} +- {:4.3f}, {:7.3f} +- {:4.3f}, {:8.3f} +- {:4.3f}, {:8.3f} +- {:4.3f}'.format(\n",
    "            ch,\n",
    "            coeffs[ch][0], u_coeffs[ch][0],\n",
    "            coeffs[ch][1], u_coeffs[ch][1],\n",
    "            coeffs[ch][2], u_coeffs[ch][2],\n",
    "            coeffs[ch][3], u_coeffs[ch][3])\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now, determine compatibility of the fit parameters, defined for two values $A, B$ by $t = \\frac{|A-B|}{\\sqrt{\\delta A^2 + \\delta B^2}}$. This is the relative error of the difference $A-B$. Thus, we consinder the values compatible on a CL of 95% (resp. 99%) if $t\\leq2$ (resp. $t\\leq3$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
   ],
   "source": [
    "t_M_Z = abs(np.array(list(coeffs.values()))[:, 0][:, None]\n",
    "            - np.array(list(coeffs.values()))[:, 0][None, :]) / (\n",
    "    np.sqrt(np.array(list(u_coeffs.values()))[:, 0][:, None]**2\n",
    "            + np.array(list(u_coeffs.values()))[:, 0][None, :]**2))\n",
    "\n",
    "t_Gamma_Z = abs(np.array(list(coeffs.values()))[:, 1][:, None]\n",
    "                - np.array(list(coeffs.values()))[:, 1][None, :]) / (\n",
    "    np.sqrt(np.array(list(u_coeffs.values()))[:, 1][:, None]**2\n",
    "            + np.array(list(u_coeffs.values()))[:, 1][None, :]**2))\n",
    "\n",
    "print(\"Resonance mass compatibility: \\n\",\n",
    "      np.array_repr(np.tril(t_M_Z),\n",
    "                    precision=2,\n",
    "                    suppress_small=True),\n",
    "      '\\n')\n",
    "\n",
    "print(\"Resonance width compatibility: \\n\",\n",
    "      np.array_repr(np.tril(t_Gamma_Z),\n",
    "                    precision=2,\n",
    "                    suppress_small=True),\n",
    "      '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "### use values of quarks\n",
    "M_Z = coeffs['qq'][0]\n",
    "u_M_Z = u_coeffs['qq'][0]\n",
    "\n",
    "Gamma_Z = coeffs['qq'][1]\n",
    "u_Gamma_Z = u_coeffs['qq'][1]\n",
    "\n",
    "# literature values (2020)\n",
    "M_Z_PDG = 91.1876\n",
    "u_M_Z_PDG = 0.0021\n",
    "\n",
    "Gamma_Z_PDG = 2.4952\n",
    "u_Gamma_Z_PDG = 0.0023\n",
    "\n",
    "# compatibility\n",
    "t_M_Z = abs(M_Z-M_Z_PDG)/np.sqrt(u_M_Z**2 + u_M_Z_PDG**2)\n",
    "t_Gamma_Z = abs(Gamma_Z-Gamma_Z_PDG)/np.sqrt(u_Gamma_Z**2 + u_Gamma_Z_PDG**2)\n",
    "\n",
    "\n",
    "# print results\n",
    "print('M_Z:     {:7.3f} +- {:6.3f}, rel. error {:9.6f}; PDG: {:7.3f} +- {:6.3f}, t = {:1.2f}'.format(\n",
    "        M_Z, u_M_Z, u_M_Z/M_Z,\n",
    "        M_Z_PDG, u_M_Z_PDG, t_M_Z))\n",
    "print('Gamma_Z: {:7.3f} +- {:6.3f}, rel. error {:9.6f}; PDG: {:7.3f} +- {:6.3f}, t = {:1.2f}'.format(\n",
    "        Gamma_Z, u_Gamma_Z, u_Gamma_Z/Gamma_Z,\n",
    "        Gamma_Z_PDG, u_Gamma_Z_PDG, t_Gamma_Z))\n",
    "\n",
    "\n",
    "### Claculate partial widths\n",
    "\n",
    "# peak hight in 1/GeV^2\n",
    "nb_to_inv_GeV_sqrd = 1e-19 / ((sc.c*sc.hbar)/sc.e)**2  # unit conversion\n",
    "Peak = {}\n",
    "u_Peak = {}\n",
    "for ch in dist_channels:\n",
    "    Peak[ch] = nb_to_inv_GeV_sqrd*coeffs[ch][2]\n",
    "    u_Peak[ch] = nb_to_inv_GeV_sqrd*u_coeffs[ch][2]\n",
    "\n",
    "# theoretical prediction (from instructions)\n",
    "part_width_theory = {\n",
    "    'ee' : 0.0838,\n",
    "    'mm' : 0.0838,\n",
    "    'tt' : 0.0838,\n",
    "    'qq' : 2*0.299+3*0.378,\n",
    "    'nu' : 0.1676}\n",
    "\n",
    "# dictionary for values\n",
    "part_width = {}\n",
    "u_part_width = {}\n",
    "\n",
    "# for electrons different formula (since input=output)\n",
    "part_width['ee'] = M_Z * Gamma_Z * np.sqrt(Peak['ee']/(12*np.pi))\n",
    "u_part_width['ee'] = part_width['ee'] * np.sqrt((u_M_Z/M_Z)**2\n",
    "                                                + (u_Gamma_Z/Gamma_Z)**2\n",
    "                                                + (0.5*u_Peak['ee']/Peak['ee'])**2)\n",
    "\n",
    "# for other channels\n",
    "for ch in dist_channels[1:]:\n",
    "    part_width[ch] = M_Z**2 * Gamma_Z**2 * Peak[ch] / (12 * np.pi * part_width['ee'])\n",
    "    u_part_width[ch] = part_width[ch] * np.sqrt((2*u_M_Z/M_Z)**2\n",
    "                                                + (2*u_Gamma_Z/Gamma_Z)**2\n",
    "                                                + (u_Peak[ch]/Peak[ch])**2\n",
    "                                                + (u_part_width['ee']/part_width['ee'])**2)\n",
    "\n",
    "# compatibility\n",
    "t_part_width = {}\n",
    "for ch in dist_channels:\n",
    "    t_part_width[ch] = abs(part_width[ch] - part_width_theory[ch]) / u_part_width[ch]\n",
    "\n",
    "# print results\n",
    "print('\\npartial widths [MeV]')\n",
    "for ch in dist_channels:\n",
    "    print('{:5s}: {:6.1f} +- {:5.1f}, rel. error {:5.4f}; theory: {:6.1f}, t={:1.2f}'.format(\n",
    "            ch,\n",
    "            part_width[ch]*1000,\n",
    "            u_part_width[ch]*1000,\n",
    "            u_part_width[ch]/part_width[ch],\n",
    "            part_width_theory[ch]*1000,\n",
    "            t_part_width[ch]))\n",
    "\n",
    "\n",
    "### Determine Number of neutrino families\n",
    "\n",
    "# invisible decay width\n",
    "Gamma_inv = Gamma_Z - sum(list(part_width.values()))\n",
    "u_Gamma_inv = np.sqrt(u_Gamma_Z**2 +\n",
    "                      np.sum(np.array(list(u_part_width.values()))**2))\n",
    "\n",
    "# neutrino number (using theoretical prediction for Gamma_nu)\n",
    "neutrino_number = Gamma_inv/part_width_theory['nu']\n",
    "u_neutrino_number = neutrino_number * u_Gamma_inv/Gamma_inv\n",
    "\n",
    "# print results\n",
    "print('\\n')\n",
    "print('Gamma invisible [MeV]:      {:6.1f} +- {:5.1f}, rel. error {:3.2f}'.format(\n",
    "        Gamma_inv*1000,\n",
    "        u_Gamma_inv*1000,\n",
    "        u_Gamma_inv/Gamma_inv))\n",
    "\n",
    "print('Number of neutrino families:   {:2.1f} +- {:2.1f}, rel. error {:3.2f}'.format(\n",
    "        neutrino_number,\n",
    "        u_neutrino_number,\n",
    "        u_neutrino_number/neutrino_number))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Exercise 4: Forward-backward asymmetry and $\\sin^2(\\theta_\\text{W})$ in muon final states\n",
    "\n",
    "* Using the **muon channel only**, measure the forward-backward asymmetry $\\mathcal{A}_\\text{FB}$ using OPAL data and muon Monte Carlo events. Take into account the radiation corrections given below. \n",
    "\n",
    "| $\\sqrt{s}$   \\[GeV\\]| Radiation correction [-]|  \n",
    "| --- | --- | \n",
    "| 88.47 | 0.021512  | \n",
    "| 89.46 | 0.019262  | \n",
    "| 90.22 | 0.016713  | \n",
    "| 91.22 | 0.018293  | \n",
    "| 91.97 | 0.030286  | \n",
    "| 92.96 | 0.062196  | \n",
    "| 93.76 | 0.093850  | \n",
    "\n",
    "Feel free to use the dictionary 'radiation_corrections' given below.\n",
    "\n",
    "* Measure the **Weinberg angle** as $\\sin^2(\\theta_\\text{W})$. Compare the measurement with the literature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "radiation_corrections = { 'energy' : [ 88.47, 89.46, 90.22, 91.22, 91.97, 92.96, 93.76] ,\n",
    "                          'correction' : [0.021512, 0.019262, 0.016713, 0.018293, 0.030286, 0.062196, 0.093850]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# In the following consider only MUON-events:\n",
    "# First, plot the cos_theta distribution for the mc- data and the opal data for Elep=45.62 GeV\n",
    "array_E_lep_mm = {}\n",
    "\n",
    "# select in opal data for muon events\n",
    "for i in range(7):\n",
    "    array_E_lep_mm[i] = {}  # sub dictionary for different variables\n",
    "    # create muon mask\n",
    "    mask = identify(array_E_lep[i],'mm')\n",
    "\n",
    "    # mask arrays of all variables\n",
    "    for variable in variable_names:\n",
    "        array_E_lep_mm[i][variable] = array_E_lep[i][variable][mask]\n",
    "\n",
    "# define variable and channel\n",
    "variable = 'cos_thet'\n",
    "channel = 'mm'\n",
    "\n",
    "# bins for the histogram\n",
    "bins_mc[variable] = np.linspace(-1,1, 201)\n",
    "\n",
    "### Plotting\n",
    "fig, ax = plt.subplots(1,2, figsize=(14, 5))\n",
    "\n",
    "# Plot monte carlo data\n",
    "ax[0].hist(array_mm[variable], bins=bins_mc[variable], histtype='step',\n",
    "           linewidth=2, color=color[channel], label=plot_label[channel])\n",
    "\n",
    "# Plot opal data\n",
    "for j in [3]:\n",
    "    ax[1].hist(array_E_lep_mm[j][variable], bins=bins_mc[variable], histtype='step',\n",
    "               linewidth=2, label=r'$\\mu$, ' +str(E_lep[j].round(2)) + ' GeV')\n",
    "\n",
    "\n",
    "# Plot settings\n",
    "ax[0].set_title(r'Cos$\\theta$ - distribution, Monte Carlo-data')\n",
    "ax[1].set_title(r'Cos$\\theta$ - distribution, Opal-data')\n",
    "ax[0].legend()\n",
    "ax[1].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Function to determine weinberg-angle out of the forward backward asymmetry\n",
    "# Simplification only valid for Elep \\approx 45.6 GeV\n",
    "def sin2_w(A):\n",
    "    return (1-np.sqrt(A/3)) * 1/4\n",
    "\n",
    "# Data managment for forward backward asymmetry\n",
    "A_fb = {}\n",
    "W_angle = {}\n",
    "\n",
    "# Define maximal angle where the data will be cut\n",
    "# Reference: \"Zedometry\", p.26\n",
    "max_angle = 0.95\n",
    "\n",
    "# Beam energies are near the resonance\n",
    "print(r'E_lep_MC   =', array['mm']['E_lep'][0].round(3), 'GeV')\n",
    "print(r'E_lep_OPAL =', E_lep[3].round(3), 'GeV\\n')\n",
    "\n",
    "### First, analyze MC data\n",
    "\n",
    "# forward, backward and total event numbers\n",
    "A_f = np.sum((array_mm['cos_thet']>=0) * (array_mm['cos_thet']<=max_angle))\n",
    "A_b = np.sum((array_mm['cos_thet']>=-max_angle) * (array_mm['cos_thet']<=0))\n",
    "N_fb = (A_f+A_b)\n",
    "\n",
    "# Calculate forward-backward asymmetry\n",
    "a_fb = (A_f-A_b) / N_fb\n",
    "u_a_fb = np.sqrt((np.sqrt(A_f)/N_fb)**2\n",
    "                 + (np.sqrt(A_b)/N_fb)**2\n",
    "                 + ((A_f-A_b)*np.sqrt(N_fb)/N_fb**2)**2)\n",
    "\n",
    "# Calculate Weinberg angle and uncertainty\n",
    "w_angle = sin2_w(a_fb)\n",
    "u_w_angle = 1/(8*np.sqrt(3)) * 1/np.sqrt(a_fb) * u_a_fb\n",
    "\n",
    "# Fill the calculated values into dictionary\n",
    "A_fb['mc'] = (a_fb, u_a_fb)\n",
    "W_angle['mc'] = (w_angle, u_w_angle)\n",
    "\n",
    "\n",
    "### Analogously for OPAL data\n",
    "\n",
    "# Create subdictionaries for opal data at different Elep\n",
    "A_fb['opal'] = {}\n",
    "W_angle['opal'] = {}\n",
    "\n",
    "# Iterate through all Elep energies\n",
    "for i in range(7):\n",
    "\n",
    "    # forward, backward and total event number\n",
    "    A_f = np.sum((array_E_lep_mm[i]['cos_thet']>0) * (array_E_lep_mm[i]['cos_thet']<max_angle))\n",
    "    A_b = np.sum((array_E_lep_mm[i]['cos_thet']>-max_angle) * (array_E_lep_mm[i]['cos_thet']<0))\n",
    "    N_fb = (A_f+A_b)\n",
    "\n",
    "    # Asymmetry\n",
    "    a_fb = (A_f-A_b) / N_fb\n",
    "    u_a_fb = np.sqrt((np.sqrt(A_f)/N_fb)**2\n",
    "                     + (np.sqrt(A_b)/N_fb)**2\n",
    "                     + ((A_f-A_b)*np.sqrt(N_fb)/N_fb**2)**2)\n",
    "    # Correct for radiation\n",
    "    a_fb = a_fb + radiation_corrections['correction'][i]\n",
    "\n",
    "    # sin^2(Weinberg angle)\n",
    "    w_angle = sin2_w(a_fb)\n",
    "    u_w_angle = 1/(8*np.sqrt(3)) * 1/np.sqrt(a_fb) * u_a_fb\n",
    "\n",
    "    # Save results in dictionary\n",
    "    A_fb['opal'][i] = (a_fb, u_a_fb)\n",
    "    W_angle['opal'][i] = (w_angle, u_w_angle)\n",
    "\n",
    "# literature values\n",
    "W_angle_PDG = 0.23122\n",
    "u_W_angle_PDG = 0.00004\n",
    "\n",
    "#compatibility between our OPAL and literature value\n",
    "t_W_angle = abs(W_angle_PDG - W_angle['opal'][3][0]) / (\n",
    "        np.sqrt(u_W_angle_PDG**2 + W_angle['opal'][3][1]**2))\n",
    "\n",
    "print('            A_fb      , rel. err.;        sin^2(theta_W) , rel. error')\n",
    "print('MC:   {:5.4f} +- {:5.4f}, {:5.4f}; {:15.4f} +- {:5.4f}, {:5.4f}'.format(\n",
    "        A_fb['mc'][0],\n",
    "        A_fb['mc'][1],\n",
    "        A_fb['mc'][1]/A_fb['mc'][0],\n",
    "        W_angle['mc'][0],\n",
    "        W_angle['mc'][1],\n",
    "        W_angle['mc'][1]/W_angle['mc'][0]))\n",
    "\n",
    "print('OPAL: {:5.4f} +- {:5.4f}, {:5.4f}; {:15.4f} +- {:5.4f}, {:5.4f}'.format(\n",
    "        A_fb['opal'][3][0],\n",
    "        A_fb['opal'][3][1],\n",
    "        A_fb['opal'][3][1]/A_fb['opal'][3][0],\n",
    "        W_angle['opal'][3][0],\n",
    "        W_angle['opal'][3][1],\n",
    "        W_angle['opal'][3][1]/W_angle['opal'][3][0]))\n",
    "\n",
    "print('PDG:                          ; {:15.4f} +- {:5.4f}, {:5.4f}; compatibility to OPAL: t = {:3.2f}'.format(\n",
    "        W_angle_PDG,\n",
    "        u_W_angle_PDG,\n",
    "        u_W_angle_PDG/W_angle_PDG, t_W_angle))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Plotting the forward-backward asymmetry\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# plot monte carlo data point\n",
    "ax.plot(array['mm']['E_lep'][0], A_fb['mc'][0], marker = '.', color ='C2', label='mc data')\n",
    "\n",
    "# plot opal data\n",
    "for i in range(7):\n",
    "    if i==1:\n",
    "        ax.errorbar(E_lep[i], A_fb['opal'][i][0], yerr=A_fb['opal'][i][1],\n",
    "                    capsize=1, fmt='.', color ='C1', label='opal data')\n",
    "    else:\n",
    "        ax.errorbar(E_lep[i], A_fb['opal'][i][0], yerr=A_fb['opal'][i][1],\n",
    "                    capsize=1, fmt='.', color ='C1')\n",
    "\n",
    "\n",
    "# Plot settings\n",
    "ax.set_xlabel(r'E_lep [GeV]')\n",
    "ax.set_ylabel(r'$A_{FB}$')\n",
    "ax.set_title(r'Forward-backward asymmetry')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Exercise 5: Tests on lepton universality\n",
    "\n",
    "* Test the lepton universality from the total cross sectinos on the peak for $Z\\to e^+ e^-$, $Z\\to \\mu^+ \\mu^-$ and $Z\\to \\tau^+ \\tau^-$ events. What is the ratio of the total cross section of the hadronic channel to the leptonic channels on the peak? Compare with the ratios obtained from the branching rations and discuss possible differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "### analyse leptonic peak hights\n",
    "\n",
    "for ch in dist_channels[:]:\n",
    "    print('Peak {:}: {:6.4f} +- {:6.4f}, rel. error: {:5.4f}'.format(\n",
    "            ch, coeffs[ch][2],\n",
    "            u_coeffs[ch][2],\n",
    "            u_coeffs[ch][2]/coeffs[ch][2]))\n",
    "\n",
    "# compatibility matrix of peak hights\n",
    "t_Peak = abs(np.array(list(coeffs.values()))[:3, 2][:, None]\n",
    "             - np.array(list(coeffs.values()))[:3, 2][None, :]) / (\n",
    "     np.sqrt(np.array(list(u_coeffs.values()))[:3, 2][:, None]**2\n",
    "             + np.array(list(u_coeffs.values()))[:3, 2][None, :]**2))\n",
    "\n",
    "\n",
    "print(\"\\nLeptons peak compatibility: \\n\",\n",
    "      np.array_repr(np.tril(t_Peak),\n",
    "                    precision=2,\n",
    "                    suppress_small=True),\n",
    "      '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "only electron and tau compatible within one sigma, tau and muon compatible within 3 sigma, muon and electron within 4 sigma. Within our uncertainties lepton universality is not fulfuilled, BUT we did not yet consider systematic uncertainties (except for luminosity). ALso fits are not too good for anything else then tau. This deviation is explainable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "### comparison to branching ratios from literature (PDG)\n",
    "\n",
    "# ratios Gamma_qq/Gamma_xx from PDG\n",
    "R_PDG = {'ee' : 20.804,\n",
    "         'mm' : 20.785,\n",
    "         'tt' : 20.764}\n",
    "\n",
    "u_R_PDG = {'ee' : 0.050,\n",
    "           'mm' : 0.033,\n",
    "           'tt' : 0.045}\n",
    "\n",
    "# ratios of peak hights\n",
    "R = {}\n",
    "u_R = {}\n",
    "\n",
    "#compatibility\n",
    "t_R = {}\n",
    "\n",
    "for ch in dist_channels[:3]:\n",
    "    R[ch] = coeffs['qq'][2]/coeffs[ch][2]\n",
    "    u_R[ch] = R[ch] * np.sqrt((u_coeffs[ch][2]/coeffs[ch][2])**2\n",
    "                              + (u_coeffs['qq'][2]/coeffs['qq'][2])**2)\n",
    "\n",
    "    t_R[ch] = abs(R_PDG[ch]-R[ch]) / np.sqrt(u_R[ch]**2 + u_R_PDG[ch]**2)\n",
    "\n",
    "print('           our OPAL                            PDG                      compatibility')\n",
    "for ch in dist_channels[:3]:\n",
    "    print('qq/{:2s}: {:4.2f} +- {:3.2f}, rel. error {:5.3f}; {:5.3f} +- {:4.3f}, rel. error {:5.4f}, t={:1.2f}'.format(\n",
    "            ch,\n",
    "            R[ch],\n",
    "            u_R[ch],\n",
    "            u_R[ch]/R[ch],\n",
    "            R_PDG[ch],\n",
    "            u_R_PDG[ch],\n",
    "            u_R_PDG[ch]/R_PDG[ch],\n",
    "            t_R[ch]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ROOT)",
   "language": "python",
   "name": "python3-root",
   "resource_dir": "/usr/local/share/jupyter/kernels/python3-root"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}