{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Exercise 1: Optimize lepton selection\n",
    "\n",
    "* First, print the distributions of the relevant variables for *all* the Monte Carlo samples (i.e. all the *channels* of the $Z$-boson decay to be studied). Which variables are these? Give sensible ranges to include all the events in the samples (both MC and OPAL data) \n",
    "* Do the same for **one** of the OPAL data samples (your lab assistant will decide which one you choose).\n",
    "* Describe the results.\n",
    "* Optimize the object selection by applying cuts. Make a strategy on how to proceed to find the optimal selection. which information do you need?\n",
    "* Determine the efficiency and the amount of background for each $Z$ decay channel. Use the simulated events $e^+e^-$, $\\mu^+\\mu^-$, $\\tau^+\\tau^-$ and hadrons ($qq$). Represent the result in a matrix form and think carefully about how you have to correct the measured rates. Don't forget to calculate the errors!\n",
    "* How do we estimate the statistical fluctuations per bin?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Import libraries**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "import uproot\n",
    "import awkward as ak\n",
    "import mplhep\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Open Monte Carlo samples**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We will open data and Monte Carlo samples using **uproot**. Uproot is a reader and a writer of the ROOT file format using only Python and Numpy. Unlike PyROOT and root_numpy, uproot does not depend on C++ ROOT so that no local compilation of the ROOT libraries is needed to access the data.\n",
    "\n",
    "You can find more info on uproot following the references:\n",
    "* Github repo: https://github.com/scikit-hep/uproot4\n",
    "* Tutorial: https://masonproffitt.github.io/uproot-tutorial/\n",
    "* Video tutorial on uproot and awkward arrays:  https://www.youtube.com/embed/ea-zYLQBS4U \n",
    "\n",
    "First, let's specify the folder path for both data and Monte Carlo (MC) samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Read the Monte Carlo data\n",
    "path_mc = 'opal_data/mc/'\n",
    "\n",
    "# Open the files\n",
    "file_mc_ee = uproot.open(path_mc+'ee.root')\n",
    "file_mc_mm = uproot.open(path_mc+'mm.root')\n",
    "file_mc_tt = uproot.open(path_mc+'tt.root')\n",
    "file_mc_qq = uproot.open(path_mc+'qq.root')\n",
    "\n",
    "\n",
    "# Name the ttree name\n",
    "ttree_name = 'myTTree'\n",
    "\n",
    "# Print list of 'branches' of the TTree (i.e. list of variable names)\n",
    "variable_names = file_mc_ee[ttree_name].keys()\n",
    "\n",
    "# Load branches\n",
    "branches_ee = file_mc_ee[ttree_name].arrays()\n",
    "branches_mm = file_mc_mm[ttree_name].arrays()\n",
    "branches_tt = file_mc_tt[ttree_name].arrays()\n",
    "branches_qq = file_mc_qq[ttree_name].arrays()\n",
    "branches_tot = ak.concatenate((branches_ee, branches_mm, branches_tt, branches_qq))\n",
    "\n",
    "\n",
    "# convert data into dictionaries of numpy arrays\n",
    "array_ee = {}\n",
    "array_mm = {}\n",
    "array_tt = {}\n",
    "array_qq = {}\n",
    "array_tot = {}\n",
    "\n",
    "for variable in variable_names:\n",
    "    array_ee[variable] = ak.to_numpy(branches_ee[variable])\n",
    "    array_mm[variable] = ak.to_numpy(branches_mm[variable])\n",
    "    array_tt[variable] = ak.to_numpy(branches_tt[variable])\n",
    "    array_qq[variable] = ak.to_numpy(branches_qq[variable])\n",
    "    array_tot[variable] = ak.to_numpy(branches_tot[variable])\n",
    "\n",
    "all_channels = ['tot', 'ee', 'mm', 'tt', 'qq']\n",
    "dist_channels = ['ee', 'mm', 'tt', 'qq']\n",
    "array = {'ee' : array_ee,\n",
    "         'mm' : array_mm,\n",
    "         'tt' : array_tt,\n",
    "         'qq' : array_qq,\n",
    "         'tot' : array_tot}\n",
    "\n",
    "\n",
    "# print('These are the different variables: ', variable_names)\n",
    "# print('E_lep = {0:4.2f} GeV = const.'.format(branches_ee['E_lep'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "| Variable name | Description |\n",
    "| --- | --- | \n",
    "| run| Run number |\n",
    "| event | Event number |\n",
    "| Ncharged | Number of charged tracks |\n",
    "| Pcharged | Total scalar sum of track momenta |\n",
    "| E_ecal| Total energy measured in the electromagnetic calorimeter |\n",
    "| E_hcal | Total energy measured in the hadronic calorimeter |\n",
    "| E_lep | LEP beam energy (=$\\sqrt{s}/2$) |\n",
    "| cos_thru | cosine of the polar angle between beam axis and thrust axis |\n",
    "| cos_thet | cosine of the polar angle between incoming positron and outgoing positive particle |\n",
    "\n",
    "\n",
    "For our statistical analysis the run and event number are not interesting. The beam energy $E_\\mathrm{lep}=45.64\\,\\mathrm{GeV}$ is constant for the data set. The scattering angles will not be used for identifying the channels and are subsect of analysis further below. Therefore we first concentrate only on the subset of variables Ncharged, Pcharged, E_ecal and E_hcal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
   ],
   "source": [
    "### We define set of variables of interest\n",
    "### and corresponding dictionaries of plot settings, etc.\n",
    "variables = ['Ncharged', 'Pcharged', 'E_ecal', 'E_hcal']\n",
    "\n",
    "# units corresponding to the variables\n",
    "unit = {'Ncharged' : '',\n",
    "         'Pcharged' : ' [GeV]',\n",
    "         'E_ecal' : ' [GeV]',\n",
    "         'E_hcal' : ' [GeV]'}\n",
    "\n",
    "# bins for histograms\n",
    "bins_mc = {'Ncharged' : np.linspace(0, 40, 41),\n",
    "           'Pcharged' : np.linspace(0, 120, 201),\n",
    "           'E_ecal' : np.linspace(0, 120, 201),\n",
    "           'E_hcal' : np.linspace(0, 40, 201)}\n",
    "\n",
    "# limits for the plots\n",
    "ylims_mc = {'Ncharged' : (0, 1e4),\n",
    "            'Pcharged' : (0, 1e4),\n",
    "            'E_ecal' : (0, 1e4),\n",
    "            'E_hcal' : (0, 1e4)}\n",
    "\n",
    "\n",
    "### Dictionaries concerning different channels\n",
    "plot_label = {'ee' : r'$e$',\n",
    "              'mm' : r'$\\mu$',\n",
    "              'tt' : r'$\\tau$',\n",
    "              'qq' : r'$q$',\n",
    "              'tot' : 'sum'}\n",
    "\n",
    "color = {'ee' : 'tab:blue',\n",
    "         'mm' : 'tab:orange',\n",
    "         'tt' : 'tab:green',\n",
    "         'qq' : 'tab:red',\n",
    "         'tot' : 'grey'}\n",
    "\n",
    "\n",
    "### plotting\n",
    "plt.style.use(mplhep.style.ATLAS)  # plot style of ATLAS\n",
    "fig, axes = plt.subplots(1, 4, figsize=(28, 5))\n",
    "\n",
    "# plot histograms for all variables and channels\n",
    "for (i, variable) in zip(range(4), variables):\n",
    "    for channel in all_channels:\n",
    "        axes[i].hist(array[channel][variable],\n",
    "                     bins=bins_mc[variable],\n",
    "                     histtype='step',\n",
    "                     linewidth=2,\n",
    "                     color=color[channel],\n",
    "                     label=plot_label[channel])\n",
    "\n",
    "\n",
    "    # plot settings\n",
    "    axes[i].set_ylim(ylims_mc[variable])\n",
    "    axes[i].set_title(variable)\n",
    "    axes[i].set_xlabel(variable + unit[variable])\n",
    "    axes[i].set_ylabel(r'# events $N$')\n",
    "    axes[i].legend()\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In the histograms above we see different kind of distributions for different variables and decay channels. Some distributions closely resemble a Gaussian such that a cut can easily be defined via mean and standard deviation. If for example we choose an interval of 3$\\sigma$ we keep approximately 99% of the events. Since the distributions partially overlap, this also causes many false positive events. We hope to make the filters unique by applying such cuts on multiple variables.\n",
    "\n",
    "Other distributions are not easily approximated by a Guassian. These are e.g. the Ncharged curves for $e$, $\\mu$ and $\\tau$, since they have a small mean. Also Pcharged is problematic since often the momentum is missing in the data (is this interpretation correct?), e.g., for $e$ about 50% of the events have Pcharged=0, while the rest of the distribution is centred at higher values.\n",
    "\n",
    "\n",
    "All these effects have to be considered to define meaningful cuts. Nevertheless, let us start the analysis by calculating the mean and standard deviation of all distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# data structure for mean values\n",
    "mean = {'ee' : {},\n",
    "        'mm' : {},\n",
    "        'tt' : {},\n",
    "        'qq' : {}}\n",
    "\n",
    "# data structure for standard deviation\n",
    "std = {'ee' : {},\n",
    "       'mm' : {},\n",
    "       'tt' : {},\n",
    "       'qq' : {}}\n",
    "\n",
    "# auxillary data structure for masks and masked arrays\n",
    "aux_mask = {'ee' : {},\n",
    "            'mm' : {},\n",
    "            'tt' : {},\n",
    "            'qq' : {}}\n",
    "\n",
    "aux_array = {'ee' : {},\n",
    "             'mm' : {},\n",
    "             'tt' : {},\n",
    "             'qq' : {}}\n",
    "\n",
    "\n",
    "# calculate values for all variables and channels\n",
    "for var in variables:\n",
    "    for ch in dist_channels:\n",
    "        # aux array is copied from regular array\n",
    "        aux_array[ch][var] = array[ch][var]\n",
    "\n",
    "        # Pcharged exhibits unexpected behavior:\n",
    "        # a significant number of values is 0.\n",
    "        # there are few outliers with very high values distorting the statistics\n",
    "        # therefore consider only 'regular' events to describe the distributions\n",
    "        if var == 'Pcharged':\n",
    "            # apply lower and upper cut\n",
    "            aux_mask[ch][var] = (array[ch][var] > 0.)\n",
    "            aux_mask[ch][var] *= (array[ch][var] < 120.)\n",
    "            aux_array[ch][var] = aux_array[ch][var][aux_mask[ch][var]]\n",
    "\n",
    "        # calculate statistics of possibly masked arrays\n",
    "        mean[ch][var] = aux_array[ch][var].mean()\n",
    "        std[ch][var] = aux_array[ch][var].std()\n",
    "\n",
    "\n",
    "    # print as a table\n",
    "    print(var)\n",
    "    print('    mean  +-   std, [min,      max]')\n",
    "    for ch in dist_channels:\n",
    "        print('{0}: {1:5.2f} +- {2:5.2f}, [{3:5.2f}, {4:6.2f}]'.format(\n",
    "                ch, mean[ch][var], std[ch][var],\n",
    "                aux_array[ch][var].min(), aux_array[ch][var].max()))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Apply cuts\n",
    "\n",
    "Apply cuts to the different distributions. Individual notes to the different cut selections are commented.\n",
    "The general reference where to apply a cut is: mean +- 3 sigma, to include most of all events in our selection.\n",
    "In cases of specific asymmetric distributions or something similar, a correction by eye is performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "### define cuts\n",
    "cuts = {'ee' : {},\n",
    "        'mm' : {},\n",
    "        'tt' : {},\n",
    "        'qq' : {}}\n",
    "\n",
    "cuts['ee'] = {'Ncharged' : (0, 6),\n",
    "              'Pcharged' : (mean['ee']['Pcharged'] - 3*std['ee']['Pcharged'], 100),  # asymmetry of distr. 3σ left, 1-2σ right\n",
    "              'E_ecal' : (mean['ee']['E_ecal'] - 3*std['ee']['E_ecal'],\n",
    "                          mean['ee']['E_ecal'] + 3*std['ee']['E_ecal']),\n",
    "              'E_hcal' : (0, mean['ee']['E_hcal'] + 3*std['ee']['E_hcal'])}\n",
    "\n",
    "cuts['mm'] = {'Ncharged' : (0, 4),\n",
    "              'Pcharged' : (mean['mm']['Pcharged'] - 3*std['mm']['Pcharged'],\n",
    "                            mean['mm']['Pcharged'] + 3*std['mm']['Pcharged']),\n",
    "              'E_ecal' : (0, mean['mm']['E_ecal'] + 3*std['mm']['E_ecal']),\n",
    "              'E_hcal' : (0, mean['mm']['E_hcal'] + 3*std['mm']['E_hcal'])}\n",
    "\n",
    "cuts['tt'] = {'Ncharged' : (0, 7),\n",
    "              'Pcharged' : (mean['tt']['Pcharged'] - 2*std['tt']['Pcharged'],\n",
    "                            mean['tt']['Pcharged'] + 3*std['tt']['Pcharged'] - 15),\n",
    "              'E_ecal' : (0, mean['tt']['E_ecal'] + 3*std['tt']['E_ecal'] - 5),\n",
    "              'E_hcal' : (0, mean['tt']['E_hcal'] + 3*std['tt']['E_hcal'])}\n",
    "\n",
    "cuts['qq'] = {'Ncharged' : (7, 39),\n",
    "              'Pcharged' : (mean['qq']['Pcharged'] - 2*std['qq']['Pcharged'] - 10,\n",
    "                            mean['qq']['Pcharged'] + 2*std['qq']['Pcharged'] - 4),\n",
    "              'E_ecal' : (mean['qq']['E_ecal'] - 3*std['qq']['E_ecal'],\n",
    "                          mean['qq']['E_ecal'] + 3*std['qq']['E_ecal'] - 10),\n",
    "              'E_hcal' : (0, mean['qq']['E_hcal'] + 3*std['qq']['E_hcal'])}\n",
    "\n",
    "\n",
    "\n",
    "# variable = 'Ncharged'\n",
    "# apply leptonic-cuts only at the right side of the distr. to distinguish leptonic and hadronic decays\n",
    "# ee: cut is a bit higher than 3sigma, correction via physical intuition\n",
    "\n",
    "# cut the hadronic distribution at both sides to distinguish from leptonic decays and background noise\n",
    "\n",
    "# variable = 'Pcharged'\n",
    "# In a significant number of ee, mm and some tt events, the momenta were not tracked! -> Pcharged = 0 has to be taken into account\n",
    "# ee: asymmetry of distribution -> 3sigma to the left, approx. 1.5sigma to the right\n",
    "# mm: 3sigma to both sides\n",
    "# tt: asymmetry of distribution ->  2sigma to the left, 3sigma to the right\n",
    "# qq: wide distribution, high std. -> 2sig to the left, 2sig to the right\n",
    "\n",
    "# Print the cuts and compare with our first guesses from the opal data\n",
    "\n",
    "# print as a table\n",
    "for var in variables:\n",
    "    print(var)\n",
    "    print('    left cut,  right cut')\n",
    "    for ch in dist_channels:\n",
    "        print('{0}: {1:7.2f} , {2:10.2f}'.format(\n",
    "                ch,\n",
    "                cuts[ch][var][0],\n",
    "                cuts[ch][var][1]))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "### Plotting only specific channels and cuts for all variables\n",
    "spec_channels = ['tot', 'ee', 'mm', 'tt', 'qq']\n",
    "spec_cuts = ['ee']\n",
    "\n",
    "print(f'data from: {spec_channels}')\n",
    "print(f'cuts from: {spec_cuts}')\n",
    "\n",
    "\n",
    "### plotting\n",
    "plt.style.use(mplhep.style.ATLAS)  # plot style of ATLAS\n",
    "fig, axes = plt.subplots(1, 4, figsize=(28, 5))\n",
    "\n",
    "# plot histograms for all variables and channels\n",
    "for (i, variable) in zip(range(4), variables):\n",
    "    for channel in spec_channels:\n",
    "        axes[i].hist(array[channel][variable],\n",
    "                     bins=bins_mc[variable],\n",
    "                     histtype='step',\n",
    "                     linewidth=2,\n",
    "                     alpha= 0.9,\n",
    "                     color=color[channel],\n",
    "                     label=plot_label[channel])\n",
    "\n",
    "    for cut in spec_cuts:\n",
    "        axes[i].axvline(cuts[cut][variable][0], ls='--', lw=3, color=color[cut])\n",
    "        axes[i].axvline(cuts[cut][variable][1], ls='--', lw=3, color=color[cut],\n",
    "                        label=f'cut: {plot_label[cut]}')\n",
    "\n",
    "\n",
    "    # plot settings\n",
    "    axes[i].set_ylim(ylims_mc[variable])\n",
    "    axes[i].set_title(variable)\n",
    "    axes[i].set_xlabel(variable + unit[variable])\n",
    "    axes[i].set_ylabel(r'# events $N$')\n",
    "    axes[i].legend()\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In the plots, specific cuts in combination with individual data can be observed.\n",
    "- The parameter \"spec_channels\" specifies the data sets that will be plotted\n",
    "- The parameter 'spec_cuts' specifies the specific particle types (ee, mm, tt or qq) for which the selection cuts will be shown\n",
    "\n",
    "This piece of code can be used to play a bit around with the data sets and the cuts.\n",
    "\n",
    "It can be observed, that:\n",
    "- Ncharged is a good indicator to seperate the hadronic from the leptonic channels\n",
    "\n",
    "- Pcharged is generally a good indicator to distinguish between mm and tt decays.\n",
    "- The problem here is, that in many cases the momenta were not tracked correctly, resulting in a tracked value of Pcharged=0.\n",
    "- The mm- respectively tt- Pcharged=0 events can not be distinguished.\n",
    "- As there is no other effective criterion to prevent mm-events to be classified as tt-events, the selection rate here is the worst in our whole data.\n",
    "- It is possible to prevent tt-events to be classified as mm-events via Ncharged. Unfortunately this is not possible the other way round.\n",
    "\n",
    "- Eecal allows us to seperate ee and mm events very effectively\n",
    "\n",
    "- Hcal can be used to avoid mistakenly assigning electrons to other events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def identify(array_xx, channel):\n",
    "    '''Returns a mask designed to identify events of channel in array_xx'''\n",
    "    mask = True\n",
    "    for variable in variables:\n",
    "        # lower and upper cut\n",
    "        mask *= array_xx[variable] >= cuts[channel][variable][0]\n",
    "        mask *= array_xx[variable] <= cuts[channel][variable][1]\n",
    "        # to include data with artifacts in 'Pcharged'\n",
    "        # this does not ocurr for hadrons\n",
    "        if variable=='Pcharged' and channel in ['ee', 'mm', 'tt']:\n",
    "            mask += array_xx['Pcharged'] == 0\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# calculate efficiency matrix\n",
    "filter_eff = np.zeros((4, 4))\n",
    "filter_error = np.zeros((4, 4))\n",
    "n_mc = np.zeros((4, 4))\n",
    "N_mc = np.zeros(4)\n",
    "\n",
    "for (j, col_ch) in zip(range(4), dist_channels):\n",
    "    N_mc[j] = len(array[col_ch]['Pcharged'])  # 100000?\n",
    "    for (i, row_ch) in zip(range(4), dist_channels):\n",
    "        n_mc[i, j] = sum(identify(array[col_ch], row_ch))\n",
    "\n",
    "\n",
    "filter_eff = n_mc / N_mc[None,:]\n",
    "filter_error = np.sqrt((n_mc+1)*(n_mc+2)/(N_mc[None,:]+2)/(N_mc[None,:]+3) - (n_mc+1)**2/(N_mc[None,:]+2)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "print(\"efficiency matrix: \\n\", np.array_repr(filter_eff, precision=7, suppress_small=True), '\\n \\n',\n",
    "      \"errors of efficiency matrix: \\n\", np.array_repr(filter_error, precision=7, suppress_small=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Matrix Inversion\n",
    "To determine the uncertainties of the matrix elements after the inversion we diffierent methods and compare:\n",
    "- analytical formula (see reference below)\n",
    "- generate random efficiency matrices, invert all and determine the statistics via:\n",
    "  * direct formula of mean and std\n",
    "  * Gaussian fit to each matrix element\n",
    "\n",
    "**References**:\n",
    "* Propagation of Errors for Matrix Inversion: https://arxiv.org/abs/hep-ex/9909031v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "### invert matrix and propagate errors\n",
    "inv_filter_eff = np.linalg.inv(filter_eff)\n",
    "inv_filter_error = np.sqrt(np.linalg.multi_dot([inv_filter_eff**2,\n",
    "                                                filter_error**2,\n",
    "                                                inv_filter_eff**2]))\n",
    "\n",
    "# covariance matrix of the matrix elements\n",
    "inv_filter_cov = np.zeros((4, 4, 4, 4))\n",
    "\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        for k in range(4):\n",
    "            for l in range(4):\n",
    "                left_vec = (inv_filter_eff[i, :]*inv_filter_eff[k, :])\n",
    "                right_vec = (inv_filter_eff[:, j]*inv_filter_eff[:, l])\n",
    "                inv_filter_cov[i, j, k, l] = left_vec @ filter_error[:, :]**2 @ right_vec\n",
    "\n",
    "\n",
    "print(inv_filter_cov[0, :, 3, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "### Number of toy experiments to be done\n",
    "ntoy = 1000\n",
    "\n",
    "### Create numpy matrix of list to append elements of inverted toy matrices\n",
    "inverse_toys = np.empty((4,4))\n",
    "\n",
    "# Create toy efficiency matrix out of gaussian-distributed random values\n",
    "for i in range(0,ntoy,1):\n",
    "    toy_matrix = np.zeros((4,4))\n",
    "    toy_matrix = np.random.normal(filter_eff,\n",
    "                                  filter_error,\n",
    "                                  size=(4,4))\n",
    "    \n",
    "    # Invert toy matrix\n",
    "    inverse_toy = np.linalg.inv(toy_matrix)\n",
    "    \n",
    "    # Append values\n",
    "    inverse_toys = np.dstack((inverse_toys,inverse_toy))\n",
    "\n",
    "### calculate the statistics directly\n",
    "inv_filter_eff_stat = np.mean(inverse_toys, axis=2)\n",
    "inv_filter_error_stat = np.std(inverse_toys, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "### Get statistics of toy experiments from Gaussian fit\n",
    "\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "\n",
    "inv_filter_eff_fit = np.zeros((4,4))\n",
    "inv_filter_error_fit = np.zeros((4,4))\n",
    "\n",
    "# Define gaussian function to fit to the toy distributions:\n",
    "def gauss(x, A, mu, sigma):\n",
    "    return A*np.exp(-(x-mu)**2/(2.*sigma**2))\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(20, 10), dpi=80)\n",
    "fig.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.2, hspace=0.2)\n",
    "ax00 = plt.subplot(4,4,1)\n",
    "ax01 = plt.subplot(4,4,2)\n",
    "ax02 = plt.subplot(4,4,3)\n",
    "ax03 = plt.subplot(4,4,4)\n",
    "\n",
    "ax10 = plt.subplot(4,4,5)\n",
    "ax11 = plt.subplot(4,4,6)\n",
    "ax12 = plt.subplot(4,4,7)\n",
    "ax13 = plt.subplot(4,4,8)\n",
    "\n",
    "ax20 = plt.subplot(4,4,9)\n",
    "ax21 = plt.subplot(4,4,10)\n",
    "ax22 = plt.subplot(4,4,11)\n",
    "ax23 = plt.subplot(4,4,12)\n",
    "\n",
    "ax30 = plt.subplot(4,4,13)\n",
    "ax31 = plt.subplot(4,4,14)\n",
    "ax32 = plt.subplot(4,4,15)\n",
    "ax33 = plt.subplot(4,4,16)\n",
    "\n",
    "axes = [[ax00,ax01,ax02,ax03],\n",
    "        [ax10,ax11,ax12,ax13],\n",
    "        [ax20,ax21,ax22,ax23],\n",
    "        [ax30,ax31,ax32,ax33]]\n",
    "\n",
    "## Find suitable ranges to fit/plot gaussian distributions\n",
    "ranges = np.zeros((4, 4, 2))\n",
    "ranges[:, :, 0] =  inv_filter_eff - 4*inv_filter_error\n",
    "ranges[:, :, 1] =  inv_filter_eff + 4*inv_filter_error\n",
    "\n",
    "## Find suitable initial parameters for gaussian distributions\n",
    "p0s = np.zeros((4, 4, 3))\n",
    "p0s[:, :, 0] = 10000\n",
    "p0s[:, :, 1] = inv_filter_eff\n",
    "p0s[:, :, 2] = inv_filter_error\n",
    "\n",
    "\n",
    "# Fill histograms for each inverted matrix coefficient:\n",
    "for j in range(0, 4):\n",
    "    for k in range(0, 4):\n",
    "\n",
    "        # Diagonal and off-diagonal terms have different histogram ranges\n",
    "        hbins, hedges, _ = axes[j][k].hist(inverse_toys[j,k,:],\n",
    "                                           bins=30,\n",
    "                                           range=ranges[j, k, :],\n",
    "                                           histtype='step',\n",
    "                                           linewidth=2,\n",
    "                                           label=f'toyhist{j}{k}')\n",
    "        axes[j][k].legend()\n",
    "\n",
    "\n",
    "        # Get the fitted curve\n",
    "        h_mid = 0.5*(hedges[1:] + hedges[:-1]) #Calculate midpoints for the fit\n",
    "        coeffs, _ = curve_fit(gauss, h_mid, hbins, p0=p0s[j, k], maxfev=10000)\n",
    "        h_fit = gauss(h_mid, *coeffs)\n",
    "\n",
    "        inv_filter_eff_fit[j, k] = coeffs[1]\n",
    "        inv_filter_error_fit[j, k] = coeffs[2]\n",
    "\n",
    "        axes[j][k].plot(h_mid, h_fit,label=f'Fit{j}{k}')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "### comparison\n",
    "print(\"inverse matrix: \\n\", np.array_repr(inv_filter_eff, precision=7, suppress_small=True), '\\n')\n",
    "print(\"analytical errors: \\n\", np.array_repr(inv_filter_error, precision=7, suppress_small=True), '\\n')\n",
    "print(\"direct statistical errors: \\n\", np.array_repr(inv_filter_error_stat, precision=7, suppress_small=True), '\\n')\n",
    "print(\"fit errors: \\n\", np.array_repr(inv_filter_error_fit, precision=7, suppress_small=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The analytical expression is exact as discussed in the reference. We see that the fit method closely matches these results. The other method however does not. Why is that so? We do not know. BUT: Since the analytical expression is exact, we do not have to find the reason. We stick with the exact propagation formula."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Exercise 2: Separate $t$- and $s$-channel contributions\n",
    "\n",
    "Only Feynman diagrams contributing to the production of $Z$ boson are to be considered for the measurements. The **electron** Monte Carlo sample incorporate contributions from $t$- and $s$-channels.\n",
    "* Select/correct contributions producing $Z$ boson decays. (Hint: Which role does the $\\cos(\\theta)$ distribution play in separating $t$- and $s$-channels?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "### We define set of angle variables\n",
    "# add definitions to dictionaries from above\n",
    "angle_variables = ['cos_thru', 'cos_thet']\n",
    "\n",
    "# units corresponding to these variables\n",
    "unit['cos_thru'] = ''\n",
    "unit['cos_thet'] = ''\n",
    "\n",
    "# bins for histograms\n",
    "bins_mc['cos_thru'] = np.linspace(-1, 1, 201)\n",
    "bins_mc['cos_thet'] = np.linspace(-1, 1, 201)\n",
    "\n",
    "# limits for the plots\n",
    "ylims_mc['cos_thru'] = (0, 2e3)\n",
    "ylims_mc['cos_thet'] = (0, 2e3)\n",
    "\n",
    "\n",
    "def s_distribution(cos_thet, A):\n",
    "    return A*(1+cos_thet**2)\n",
    "\n",
    "def t_distribution(cos_thet, A):\n",
    "    return A*(1-cos_thet)**-2\n",
    "\n",
    "### plotting\n",
    "plt.style.use(mplhep.style.ATLAS)  # plot style of ATLAS\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# plot histograms for all variables and channels\n",
    "for (i, variable) in zip(range(2), angle_variables):\n",
    "    for channel in ['ee']:\n",
    "        axes[i].hist(array[channel][variable],\n",
    "                     bins=bins_mc[variable],\n",
    "                     histtype='step',\n",
    "                     linewidth=2,\n",
    "                     color=color[channel],\n",
    "                     label=plot_label[channel])\n",
    "    # if i==1:\n",
    "        # axes[i].plot(bins_mc['cos_thet'],s_distribution(bins_mc['cos_thet'], 100), label='s_channel')\n",
    "        # axes[i].plot(bins_mc['cos_thet'],t_distribution(bins_mc['cos_thet'], 20), label='t_channel')\n",
    "    \n",
    "    # plot settings\n",
    "    axes[i].set_ylim(ylims_mc[variable])\n",
    "    axes[i].set_title(variable)\n",
    "    axes[i].set_xlabel(variable + unit[variable])\n",
    "    axes[i].set_ylabel(r'# events $N$')\n",
    "    axes[i].legend()\n",
    "    \n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Way to go:\n",
    "# Cite Paper: Precise Determination of the Z Resonance Parameters at LEP: \"Zedometry\"\n",
    "# p.23 (ch. 7.2.1 t-channel contribution to ee -> ee)\n",
    "# Some key points:\n",
    "# - t-channel amplitude is non-resonant, dominated by photon exchange\n",
    "# - relative size of t-channel amplitude depends on the scattering angle (theta)\n",
    "# - cut is made at |cos(theta)| < 0.7\n",
    "# - resulting from high-statistics Monte Carlo samples\n",
    "# OPAL_Paper_3 results in the same\n",
    "\n",
    "cuts['ee']['cos_thet'] = (-0.7, 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# fit s- and t-channel distributions\n",
    "def s_t_distribution(cos_thet, A, B):\n",
    "    return A*(1+cos_thet**2) + B*(1-cos_thet)**-2\n",
    "\n",
    "def s_t(A, B):\n",
    "    return A * 1.62867 + B * 2.7451\n",
    "\n",
    "# define variable and channel\n",
    "variable = 'cos_thet'\n",
    "channel = 'ee'\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "hbins, hedges, _ = ax.hist(array[channel][variable], bins=np.linspace(-1,1,201),\n",
    "                        histtype='step', linewidth=2, color='C0',\n",
    "                        label=plot_label[channel])\n",
    "\n",
    "h_mid = 0.5*(hedges[1:] + hedges[:-1]) #Calculate midpoints for the fit\n",
    "coeffs, errors = curve_fit(s_t_distribution, h_mid[10:-10], hbins[10:-10], maxfev=10000)  # fit in between -0.9 and 0.9\n",
    "print(coeffs)\n",
    "print(errors)\n",
    "ax.plot(bins_mc[variable], s_t_distribution(bins_mc[variable], coeffs[0], coeffs[1]), color=\"C2\", label='(s+t) - fit')\n",
    "ax.plot(bins_mc[variable], s_distribution(bins_mc[variable], coeffs[0]), color=\"C3\", label='s - fit')\n",
    "ax.axvline(cuts[channel][variable][1], ls='--', color = 'C1', label=r'data cuts')\n",
    "ax.axvline(cuts[channel][variable][0], ls='--', color = 'C1')\n",
    "ax.axvline(-0.9, ls='--', color = 'black', label=r'fit limits')\n",
    "ax.axvline(0.9, ls='--', color = 'black')\n",
    "ax.set_ylim(0,1e3)\n",
    "ax.set_title(r'cos_theta-distribution')\n",
    "ax.set_xlabel(r'cos($\\theta$)')\n",
    "ax.set_ylabel(r'# events $N$')\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "# print(h_mid)\n",
    "# print(hbins)\n",
    "\n",
    "bin_width = hedges[1]-hedges[0]\n",
    "\n",
    "A_count = s_t(coeffs[0], coeffs[1]) / bin_width\n",
    "u_A_count = np.sqrt((s_t(errors[0,0], 0))**2\n",
    "                      + s_t(0, errors[1,1])**2\n",
    "                      + 2 * s_t(errors[0,0],0) * \n",
    "                      s_t(0, errors[1,1])*errors[0,1]) / bin_width\n",
    "A_real = 8/3 * coeffs[0] / bin_width\n",
    "u_A_real = 8/3 * errors[0,0] / bin_width\n",
    "ratio_s_t = A_real / A_count\n",
    "u_ratio_s_t = np.sqrt((u_A_real/A_count)**2 + (A_real/A_count**2 * u_A_count)**2)\n",
    "print('A_count =', A_count, '+-', u_A_count, 'relative error:', u_A_count/A_count)\n",
    "print('A_real =', A_real, '+-', 'relative error:', u_A_real, u_A_real/A_real)\n",
    "print('Ratio real s-channel data / counted data =', ratio_s_t, '+-', u_ratio_s_t, 'relative error:', u_ratio_s_t/ratio_s_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def identify_s(array_ee):\n",
    "    '''Returns a mask designed to identify s-channel in array_ee'''\n",
    "    mask = True\n",
    "    mask *= array_ee['cos_thet'] >= cuts['ee']['cos_thet'][0]\n",
    "    mask *= array_ee['cos_thet'] <= cuts['ee']['cos_thet'][1]\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "variable = 'Pcharged'\n",
    "mask_ee = identify(array['tot'],'ee')\n",
    "mask_thet_ee = mask_ee * identify_s(array['tot'])\n",
    "\n",
    "axes[0].hist(array['tot'][variable][mask_ee], bins=np.linspace(0,120,201), histtype='step',linewidth=2,label=r'ee-data without cos$\\theta$ - cut')\n",
    "axes[1].hist(array['tot'][variable][mask_thet_ee], bins=np.linspace(0,120,201), histtype='step',linewidth=2,label=r'ee-data with cos$\\theta$ - cut')\n",
    "axes[0].set_title(r\"Pcharged without cos$\\theta$ - cut\")\n",
    "axes[1].set_title(r\"Pcharged with cos$\\theta$ - cut\")\n",
    "axes[0].set_xlabel(variable + unit[variable])\n",
    "axes[1].set_xlabel(variable + unit[variable])\n",
    "axes[0].set_ylabel(r'# events $N$')\n",
    "axes[1].set_ylabel(r'# events $N$')\n",
    "axes[0].legend()\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Exercise 3: Measurement of the total production cross sections\n",
    "\n",
    "For **each** of the seven centre-of-mass energies:\n",
    "* Determine the number of events in the handronic channel *and* in the three leptonic channels\n",
    "* Substract the background and correct for selection efficiencies accordingly\n",
    "* Then, calculate the differnetial cross sections for the hadronic *and* the leptnic channels\n",
    "* Add the radiation corrections from The table given below. **Don't forget to take the uncertainties (errors) into account!**\n",
    "\n",
    "| $\\sqrt{s}$   \\[GeV\\]| Correction hadronic channel    \\[nb\\] |  Correction leptonic channel   \\[nb\\]|\n",
    "| --- | --- | --- |\n",
    "| 88.47 | +2.0  | +0.09 |\n",
    "| 89.46 | +4.3  | +0.20 |\n",
    "| 90.22 | +7.7  | +0.36 |\n",
    "| 91.22 | +10.8 | +0.52 |\n",
    "| 91.97 | +4.7  | +0.22 |\n",
    "| 92.96 | -0.2  | -0.01 |\n",
    "| 93.76 | -1.6  | -0.08 |\n",
    "\n",
    "Feel free to access these values using the dictionary 'xs_corrections' given below.\n",
    "* Once the total cross section for all four decay channels at all seven energies have been measured, fit a **Breit-Wigner distribution** to measure the $Z$ boson mass ($m_Z$) and the resonance width ($\\Gamma_Z$) and the peak cross section s of the resonance for the hadronic and the leptonic channels. Again, **propagate the uncertainties carefully**.\n",
    "* Compare your results to the OPAL cross section s and the theoretical predictions. How many degrees of freedom does the fit have? How can you udge if the model is compatible with the measured data? Calculate the  **confidence levels**.\n",
    "* Calculate the partial widths for all channels from the measured cross sections on the peak. Which is the best partial width to start with? Compare them with the theoretical predictions and the values that you have calculated in the beginning.\n",
    "* Determine from your results the **number of generations of light neutrinos**. Which assumptions are necessary?\n",
    "* Discuss in detail the systematic uncertainties in the whole procedure of the analysis. Which assumptions were necessary?\n",
    "\n",
    "These are some **references** that might be interesting to look up:\n",
    "* Particle Data Book: https://pdg.lbl.gov/2020/download/Prog.Theor.Exp.Phys.2020.083C01.pdf\n",
    "** Resonances: https://pdg.lbl.gov/2017/reviews/rpp2017-rev-resonances.pdf\n",
    "* Precision Electroweak Measurements on the Z Resonance (Combination LEP): https://arxiv.org/abs/hep-ex/0509008\n",
    "* Measurement of the $Z^0$ mass and width with the OPAL detector at LEP: https://doi.org/10.1016/0370-2693(89)90705-3\n",
    "* Measurement of the $Z^0$ line shape parameters and the electroweak couplings of charged leptons: https://inspirehep.net/literature/315269\n",
    "* The OPAL Collaboration, *Precise Determination of the $Z$ Resonance Parameters at LEP: \"Zedometry\"*: https://arxiv.org/abs/hep-ex/0012018\n",
    "* Fitting a Breit-Wigner curve using uproot: https://masonproffitt.github.io/uproot-tutorial/07-fitting/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "xs_corrections = { 'energy' : [88.47, 89.46, 90.22, 91.22, 91.97, 92.96, 93.76] ,\n",
    "                   'hadronic' : [2.0, 4.3, 7.7, 10.8, 4.7, -0.2, -1.6],\n",
    "                   'leptonic' : [0.09, 0.20, 0.36, 0.52, 0.22, -0.01, -0.08]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Data management\n",
    "\n",
    "path_data = 'opal_data/data'\n",
    "\n",
    "# Open the files\n",
    "file_lep = uproot.open(path_data+'/daten_1.root')\n",
    "\n",
    "# Name the ttree name\n",
    "ttree_name = 'myTTree'\n",
    "\n",
    "# Load branches\n",
    "branches_lep = file_lep[ttree_name].arrays()\n",
    "\n",
    "# convert data into dictionaries of numpy arrays\n",
    "array_lep = {}\n",
    "\n",
    "for variable in variable_names:\n",
    "    array_lep[variable] = ak.to_numpy(branches_lep[variable])\n",
    "\n",
    "N = len(array_lep['E_ecal'])\n",
    "# only 176 000 data\n",
    "print('Number of events:', N)\n",
    "\n",
    "print('These are the different variables: ', file_lep[ttree_name].keys(), ' the same as before')\n",
    "# print('E_lep = {0:4.2f} GeV = const.'.format(branches_ee['E_lep'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# identify the events of each energy\n",
    "\n",
    "# dictionary for different beam energies\n",
    "array_E_lep = {}\n",
    "E_lep = np.zeros(7)\n",
    "\n",
    "# roughly estimated bin edges\n",
    "E_bin_edges = [44, 44.5, 44.9, 45.3, 45.8, 46.2, 46.7, 47.5]\n",
    "\n",
    "for i in range(7):\n",
    "    array_E_lep[i] = {}  # sub dictionary for different variables\n",
    "    mask = True\n",
    "    mask *= (array_lep['E_lep'] >= E_bin_edges[i])\n",
    "    mask *= (array_lep['E_lep'] <= E_bin_edges[i+1])\n",
    "\n",
    "    # mask arrays of all variables\n",
    "    for variable in variable_names:\n",
    "        array_E_lep[i][variable] = array_lep[variable][mask]\n",
    "\n",
    "    E_lep[i] = np.mean(array_E_lep[i]['E_lep'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# M_all_chs contains vectors of event numbers (of all channels)\n",
    "M_all_chs = np.zeros((7, 4))\n",
    "\n",
    "for i in range(7):\n",
    "    for (j, ch) in zip(range(4), dist_channels):\n",
    "        M_all_chs[i, j] = np.sum(identify(array_E_lep[i], ch))\n",
    "\n",
    "\n",
    "print('E_lep | [ee,     mm,    tt,    qq]')\n",
    "print('------------------------------------')\n",
    "for i in range(7):\n",
    "    print(round(E_lep[i], 2), '|', M_all_chs[i, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# M are vectors of event numbers (only s channel)\n",
    "M = M_all_chs\n",
    "u_M = np.sqrt(M_all_chs)\n",
    "\n",
    "# for ee we have to separate s and t channel\n",
    "# keep only s channel and correct for missing events\n",
    "for i in range(7):\n",
    "    mask_ee_s = identify(array_E_lep[i], 'ee')\n",
    "    mask_ee_s *= identify_s(array_E_lep[i])\n",
    "    M[i, 0] = np.sum(mask_ee_s)\n",
    "    M[i, 0] *= ratio_s_t  # correct for s-channel selection\n",
    "    u_M[i, 0] = M[i, 0] * np.sqrt((u_ratio_s_t/ratio_s_t)**2 + 1/M[i, 0])  # error propagation\n",
    "    \n",
    "    \n",
    "print('E_lep | [ee,     mm,    tt,    qq]')\n",
    "print('------------------------------------')\n",
    "for i in range(7):\n",
    "    print(round(E_lep[i], 2), '|', np.round(M[i, :], 0), np.round(u_M[i, :], 0), np.round(u_M[i, :]/M[i, :], 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# N are vectors of event numbers corrected for efficiency\n",
    "N = np.zeros((7, 4))\n",
    "N_cov = np.zeros((7, 4, 4))\n",
    "u_N = np.zeros((7, 4))\n",
    "\n",
    "for i in range(7):\n",
    "    N[i, :] = inv_filter_eff @ M[i, :]\n",
    "    for j in range(4):\n",
    "        for k in range(4):\n",
    "            N_cov[i, j, k] = M[i, :] @ inv_filter_cov[j, :, k, :] @ M[i, :]\n",
    "            N_cov[i, j, k] += (inv_filter_eff[j, :] * inv_filter_eff[k, :]) @ u_M[i, :]**2\n",
    "    u_N[i, :] = np.sqrt(np.diagonal(N_cov[i, :, :]))\n",
    "\n",
    "\n",
    "print('E_lep | [ee,     mm,    tt,    qq]')\n",
    "print('------------------------------------')\n",
    "for i in range(7):\n",
    "    print(round(E_lep[i], 2), '|', np.round(N[i, :], 0), np.round(u_N[i, :], 0), np.round(u_N[i, :]/N[i, :], 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "%pip install pandas\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "lumi_df = pd.read_csv('opal_data/lumi-files/daten_1.csv','r')\n",
    "\n",
    "print(lumi_df.keys())\n",
    "\n",
    "#lumi_df['']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Exercise 4: Forward-backward asymmetry and $\\sin^2(\\theta_\\text{W})$ in muon final states\n",
    "\n",
    "* Using the **muon channel only**, measure the forward-backward asymmetry $\\mathcal{A}_\\text{FB}$ using OPAL data and muon Monte Carlo events. Take into account the radiation corrections given below. \n",
    "\n",
    "| $\\sqrt{s}$   \\[GeV\\]| Radiation correction [-]|  \n",
    "| --- | --- | \n",
    "| 88.47 | 0.021512  | \n",
    "| 89.46 | 0.019262  | \n",
    "| 90.22 | 0.016713  | \n",
    "| 91.22 | 0.018293  | \n",
    "| 91.97 | 0.030286  | \n",
    "| 92.96 | 0.062196  | \n",
    "| 93.76 | 0.093850  | \n",
    "\n",
    "Feel free to use the dictionary 'radiation_corrections' given below.\n",
    "\n",
    "* Measure the **Weinberg angle** as $\\sin^2(\\theta_\\text{W})$. Compare the measurement with the literature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Calculate the Weinberg angle using the Monte Carlo events\n",
    "array_E_lep_mm = {}\n",
    "\n",
    "for i in range(7):\n",
    "    array_E_lep_mm[i] = {}  # sub dictionary for different variables\n",
    "    mask = identify(array_E_lep[i],'mm')\n",
    "\n",
    "    # mask arrays of all variables\n",
    "    for variable in variable_names:\n",
    "        array_E_lep_mm[i][variable] = array_E_lep[i][variable][mask]\n",
    "\n",
    "variable = 'cos_thet'\n",
    "channel = 'mm'\n",
    "\n",
    "bins_mc[variable] = np.linspace(-1,1, 201)\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(14, 5))\n",
    "ax[0].hist(array_mm[variable], bins=bins_mc[variable], histtype='step', linewidth=2, color=color[channel], label=plot_label[channel])\n",
    "for j in range(7):\n",
    "    ax[1].hist(array_E_lep_mm[j][variable], bins=bins_mc[variable], histtype='step',\n",
    "               linewidth=2, label=str(E_lep[j].round(2)) + ' GeV')\n",
    "ax[0].set_title(r'Cos$\\theta$ - distribution, Monte Carlo-data')\n",
    "ax[1].set_title(r'Cos$\\theta$ - distribution, Opal-data')\n",
    "ax[0].legend()\n",
    "ax[1].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Function to determine weinberg-angle\n",
    "def sin2_w(A):\n",
    "    return (1-np.sqrt(A/3)) * 1/4\n",
    "print(sin2_w(2))\n",
    "print(1/12*(3-np.sqrt(6)))\n",
    "\n",
    "# Data managment for forward backward asymmetry\n",
    "A_fb = {}\n",
    "W_angle = {}\n",
    "\n",
    "# Analyse the monte-carlo Data\n",
    "print(r'Elep_mc =', array['mm']['E_lep'][0], 'GeV')  # Elep=45.624 GeV for Monte Carlo-data\n",
    "\n",
    "# forward term\n",
    "A_f = np.sum((array_mm['cos_thet']>0) * (array_mm['cos_thet']<1))\n",
    "# backwards term\n",
    "A_b = np.sum((array_mm['cos_thet']>-1) * (array_mm['cos_thet']<0))\n",
    "N = (A_f+A_b)\n",
    "\n",
    "a_fb = (A_f-A_b) / N  # calculate Asymmetry\n",
    "u_a_fb = np.sqrt((np.sqrt(A_f)/N)**2 + (np.sqrt(A_b)/N)**2 + (np.sqrt(N)/N**2)**2)\n",
    "\n",
    "w_angle = sin2_w(a_fb)  # calculate Weinberg-angle\n",
    "u_w_angle = 1/(8*np.sqrt(3)) * 1/np.sqrt(a_fb) * u_a_fb\n",
    "\n",
    "A_fb['mc'] = (a_fb, u_a_fb)\n",
    "W_angle['mc'] = (w_angle, u_w_angle)\n",
    "\n",
    "A_fb['opal'] = {}  # create subdictionary\n",
    "W_angle['opal'] = {}  # create subdict\n",
    "\n",
    "for i in range(7):\n",
    "    # print('E_lep=', E_lep[i])\n",
    "    A_f = np.sum((array_E_lep_mm[i]['cos_thet']>0) * (array_E_lep_mm[i]['cos_thet'])<1)\n",
    "    A_b = np.sum((array_E_lep_mm[i]['cos_thet']>-1) * (array_E_lep_mm[i]['cos_thet'])<0)\n",
    "    N = (A_f+A_b)\n",
    "\n",
    "    a_fb = (A_f-A_b) / N\n",
    "    u_a_fb = np.sqrt((np.sqrt(A_f)/N)**2 + (np.sqrt(A_b)/N)**2 + (np.sqrt(N)/N**2)**2)\n",
    "\n",
    "    w_angle = sin2_w(a_fb)\n",
    "    u_w_angle = 1/(8*np.sqrt(3)) * 1/np.sqrt(a_fb) * u_a_fb\n",
    "\n",
    "    A_fb['opal'][i] = (a_fb, u_a_fb)\n",
    "    W_angle['opal'][i] = (w_angle, u_w_angle)\n",
    "    \n",
    "print(A_fb)\n",
    "print(W_angle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "# plot monte carlo data point\n",
    "ax.plot(array['mm']['E_lep'][0], A_fb['mc'][0], marker = 'x', color ='C2')\n",
    "# plot opal data\n",
    "for i in range(7):\n",
    "    ax.plot(E_lep[i], A_fb['opal'][i][0], marker = 'x', lw=0, color ='C0')\n",
    "ax.set_xlabel(r'E_lep [GeV]')\n",
    "ax.set_ylabel(r'A_{FB}')\n",
    "ax.set_title(r'Forward-backward asymmetry')\n",
    "plt.show()\n",
    "print(E_lep*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "radiation_corrections = { 'energy' : [ 88.47, 89.46, 90.22, 91.22, 91.97, 92.96, 93.76] ,\n",
    "                          'correction' : [0.021512, 0.019262, 0.016713, 0.018293, 0.030286, 0.062196, 0.093850]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Exercise 5: Tests on lepton universality¶\n",
    "\n",
    "* Test the lepton universality from the total cross sectinos on the peak for $Z\\to e^+ e^-$, $Z\\to \\mu^+ \\mu^-$ and $Z\\to \\tau^+ \\tau^-$ events. What is the ratio of the total cross section of the hadronic channel to the leptonic channels on the peak? Compare with the ratios obtained from the branching rations and discuss possible differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ROOT)",
   "language": "python",
   "name": "python3-root",
   "resource_dir": "/usr/local/share/jupyter/kernels/python3-root"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}